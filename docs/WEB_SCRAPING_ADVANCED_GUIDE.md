# ğŸ•·ï¸ ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ë²½ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ ê¸°ìˆ ](#í•µì‹¬-ê¸°ìˆ )
3. [ëª¨ë“ˆ êµ¬ì¡°](#ëª¨ë“ˆ-êµ¬ì¡°)
4. [ì‹¤ì „ í…Œí¬ë‹‰](#ì‹¤ì „-í…Œí¬ë‹‰)
5. [Anti-Bot ìš°íšŒ](#anti-bot-ìš°íšŒ)
6. [ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ](#ì‹¤ì œ-ì‚¬ìš©-ì˜ˆì œ)
7. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ê°œìš”

web-scraping.devì—ì„œ í•™ìŠµí•œ ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ìˆ ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

### í•™ìŠµ ì†ŒìŠ¤
- **ì›¹ì‚¬ì´íŠ¸**: https://web-scraping.dev
- **ì œì‘ì**: ScrapFly
- **ëª©ì **: ì‹¤ì œ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œë‚˜ë¦¬ì˜¤ í•™ìŠµ

---

## í•µì‹¬ ê¸°ìˆ 

### 1. í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬

#### ì •ì  HTML í˜ì´ì§•
```python
def scrape_with_pagination(url_pattern: str, max_pages: int = None):
    all_data = []
    page = 1
    
    while True:
        if max_pages and page > max_pages:
            break
        
        url = url_pattern.format(page=page)
        response = session.get(url)
        
        # ë°ì´í„° ì¶”ì¶œ
        soup = BeautifulSoup(response.text, 'html.parser')
        products = soup.find_all('div', class_='product')
        
        if not products:
            break
        
        all_data.extend(products)
        page += 1
        time.sleep(0.5)  # Rate limiting
    
    return all_data
```

#### ë¬´í•œ ìŠ¤í¬ë¡¤
```python
def handle_infinite_scroll(base_url: str, max_items: int = 50):
    # API ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°
    api_url = f"{base_url}/api/items"
    
    items = []
    offset = 0
    limit = 20
    
    while len(items) < max_items:
        params = {'offset': offset, 'limit': limit}
        response = requests.get(api_url, params=params)
        data = response.json()
        
        if not data['items']:
            break
        
        items.extend(data['items'])
        offset += limit
    
    return items[:max_items]
```

#### "Load More" ë²„íŠ¼
```python
def handle_load_more(url: str):
    all_items = []
    next_url = url
    
    while next_url:
        response = requests.get(next_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì•„ì´í…œ ì¶”ì¶œ
        items = soup.find_all('div', class_='item')
        all_items.extend(items)
        
        # ë‹¤ìŒ í˜ì´ì§€ URL ì°¾ê¸°
        load_more = soup.find('button', {'data-action': 'load-more'})
        if load_more and load_more.get('data-next-url'):
            next_url = load_more['data-next-url']
        else:
            next_url = None
    
    return all_items
```

### 2. ì¸ì¦ ì²˜ë¦¬

#### ì¿ í‚¤ ê¸°ë°˜ ë¡œê·¸ì¸
```python
def login_with_cookies(login_url: str, credentials: dict):
    session = requests.Session()
    
    # ë¡œê·¸ì¸
    response = session.post(login_url, data=credentials)
    
    # ì¿ í‚¤ ì €ì¥
    cookies = session.cookies.get_dict()
    
    # ì¸ì¦ëœ í˜ì´ì§€ ì ‘ê·¼
    protected_url = "https://example.com/protected"
    response = session.get(protected_url)
    
    return response.text
```

#### CSRF í† í° ì²˜ë¦¬
```python
def handle_csrf_token(session: requests.Session, form_url: str):
    # í¼ í˜ì´ì§€ì—ì„œ CSRF í† í° ì¶”ì¶œ
    response = session.get(form_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    csrf_token = soup.find('input', {'name': 'csrf_token'})['value']
    
    # í† í°ê³¼ í•¨ê»˜ ì œì¶œ
    data = {
        'csrf_token': csrf_token,
        'username': 'user',
        'password': 'pass'
    }
    
    response = session.post(form_url, data=data)
    return response
```

#### API í‚¤ ì¸ì¦
```python
def api_with_auth(api_url: str, api_key: str):
    headers = {
        'Authorization': f'Bearer {api_key}',
        'X-API-Key': api_key
    }
    
    response = requests.get(api_url, headers=headers)
    return response.json()
```

### 3. ë°ì´í„° ì¶”ì¶œ

#### ìˆ¨ê²¨ì§„ JSON ë°ì´í„°
```python
def extract_hidden_json(html: str):
    soup = BeautifulSoup(html, 'html.parser')
    hidden_data = {}
    
    # JavaScript ë³€ìˆ˜ì—ì„œ ì¶”ì¶œ
    scripts = soup.find_all('script')
    for script in scripts:
        if script.string:
            # window.* íŒ¨í„´
            matches = re.findall(
                r'window\.(\w+)\s*=\s*({.*?});', 
                script.string, 
                re.DOTALL
            )
            for var_name, json_str in matches:
                try:
                    hidden_data[var_name] = json.loads(json_str)
                except:
                    pass
    
    # JSON-LD êµ¬ì¡°í™” ë°ì´í„°
    json_ld = soup.find_all('script', type='application/ld+json')
    for script in json_ld:
        try:
            hidden_data['structured_data'] = json.loads(script.string)
        except:
            pass
    
    return hidden_data
```

#### Local Storage ë°ì´í„°
```python
# Seleniumì„ ì‚¬ìš©í•œ Local Storage ì ‘ê·¼
from selenium import webdriver

def get_local_storage_data(url: str):
    driver = webdriver.Chrome()
    driver.get(url)
    
    # Local Storage ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    local_storage = driver.execute_script(
        "return window.localStorage;"
    )
    
    # Session Storage ë°ì´í„°
    session_storage = driver.execute_script(
        "return window.sessionStorage;"
    )
    
    driver.quit()
    
    return {
        'local': local_storage,
        'session': session_storage
    }
```

### 4. Anti-Bot ìš°íšŒ

#### User-Agent ë¡œí…Œì´ì…˜
```python
import random

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

def get_random_headers():
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
```

#### Rate Limiting
```python
import time
from functools import wraps

def rate_limit(calls_per_second=1):
    min_interval = 1.0 / calls_per_second
    
    def decorator(func):
        last_called = [0.0]
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            
            result = func(*args, **kwargs)
            last_called[0] = time.time()
            
            return result
        
        return wrapper
    
    return decorator

@rate_limit(calls_per_second=2)
def scrape_page(url):
    return requests.get(url)
```

#### Proxy ë¡œí…Œì´ì…˜
```python
import itertools

class ProxyRotator:
    def __init__(self, proxies):
        self.proxies = proxies
        self.proxy_pool = itertools.cycle(proxies)
    
    def get_next_proxy(self):
        return next(self.proxy_pool)
    
    def make_request(self, url):
        proxy = self.get_next_proxy()
        
        try:
            response = requests.get(
                url,
                proxies={'http': proxy, 'https': proxy},
                timeout=10
            )
            return response
        except:
            # ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ í”„ë¡ì‹œ ì‹œë„
            return self.make_request(url)
```

---

## ëª¨ë“ˆ êµ¬ì¡°

### AdvancedScraper í´ë˜ìŠ¤

```python
from src.scraper.advanced_scraper import AdvancedScraper

# ì´ˆê¸°í™”
scraper = AdvancedScraper(base_url="https://example.com")

# API ìŠ¤í¬ë˜í•‘
data = scraper.scrape_api('/api/products')

# í˜ì´ì§€ë„¤ì´ì…˜
products = scraper.scrape_with_pagination(
    url_pattern="https://example.com/products?page={page}",
    max_pages=5
)

# ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ
hidden = scraper.extract_hidden_data("https://example.com/product/1")

# ì¿ í‚¤ ì‚¬ìš©
scraper.scrape_with_cookies(url, cookies={'session': 'abc123'})

# Referer í—¤ë”
scraper.scrape_with_referer(url, referer='https://google.com')
```

### ScrapingOrchestrator í´ë˜ìŠ¤

```python
from src.scraper.advanced_scraper import (
    ScrapingOrchestrator, 
    ScrapingTarget, 
    ScrapingMethod
)

# ì¡°ìœ¨ì ì´ˆê¸°í™”
orchestrator = ScrapingOrchestrator()

# ëŒ€ìƒ ì¶”ê°€
orchestrator.add_target(
    ScrapingTarget(
        url="https://example.com/api/products",
        method=ScrapingMethod.API,
        has_pagination=True
    )
)

# ì‹¤í–‰
results = orchestrator.execute()
```

---

## ì‹¤ì „ í…Œí¬ë‹‰

### 1. ë™ì  ì½˜í…ì¸  ì²˜ë¦¬

#### Selenium ì‚¬ìš©
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def scrape_dynamic_content(url):
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)
    
    try:
        driver.get(url)
        
        # ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "product"))
        )
        
        # JavaScript ì‹¤í–‰
        products = driver.execute_script(
            "return document.querySelectorAll('.product')"
        )
        
        return driver.page_source
        
    finally:
        driver.quit()
```

#### Playwright ì‚¬ìš©
```python
from playwright.sync_api import sync_playwright

def scrape_with_playwright(url):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ê°€ë¡œì±„ê¸°
        def handle_request(request):
            if request.resource_type == "image":
                request.abort()
            else:
                request.continue_()
        
        page.route("**/*", handle_request)
        
        page.goto(url)
        page.wait_for_selector('.product')
        
        content = page.content()
        browser.close()
        
        return content
```

### 2. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

#### ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
```python
import requests
from lxml import etree

def stream_large_xml(url):
    response = requests.get(url, stream=True)
    parser = etree.iterparse(response.raw, events=('start', 'end'))
    
    for event, elem in parser:
        if event == 'end' and elem.tag == 'product':
            # ì œí’ˆ ì²˜ë¦¬
            process_product(elem)
            
            # ë©”ëª¨ë¦¬ ì ˆì•½
            elem.clear()
            while elem.getprevious() is not None:
                del elem.getparent()[0]
```

#### ë³‘ë ¬ ì²˜ë¦¬
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

lock = threading.Lock()
results = []

def scrape_url(url):
    response = requests.get(url)
    data = parse_response(response)
    
    with lock:
        results.append(data)
    
    return data

def parallel_scraping(urls, max_workers=10):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(scrape_url, url): url for url in urls}
        
        for future in as_completed(futures):
            url = futures[future]
            try:
                data = future.result()
                print(f"âœ… ì™„ë£Œ: {url}")
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨: {url} - {e}")
    
    return results
```

---

## Anti-Bot ìš°íšŒ

### 1. Cloudflare ìš°íšŒ

```python
# cloudscraper ì‚¬ìš©
import cloudscraper

scraper = cloudscraper.create_scraper()
response = scraper.get("https://protected-site.com")
```

### 2. reCAPTCHA ì²˜ë¦¬

```python
# 2captcha ì„œë¹„ìŠ¤ ì‚¬ìš© ì˜ˆì œ
import requests

def solve_recaptcha(site_key, page_url, api_key):
    # CAPTCHA í•´ê²° ìš”ì²­
    response = requests.post(
        'http://2captcha.com/in.php',
        data={
            'key': api_key,
            'method': 'userrecaptcha',
            'googlekey': site_key,
            'pageurl': page_url
        }
    )
    
    captcha_id = response.text.split('|')[1]
    
    # ê²°ê³¼ ëŒ€ê¸°
    time.sleep(20)
    
    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    response = requests.get(
        f'http://2captcha.com/res.php?key={api_key}&action=get&id={captcha_id}'
    )
    
    return response.text.split('|')[1]
```

### 3. ë¸Œë¼ìš°ì € í•‘ê±°í”„ë¦°íŒ… ìš°íšŒ

```python
from selenium import webdriver

def setup_anti_detection_browser():
    options = webdriver.ChromeOptions()
    
    # ìë™í™” ê°ì§€ ìš°íšŒ
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--disable-blink-features=AutomationControlled")
    
    # WebDriver ì†ì„± ìˆ¨ê¸°ê¸°
    driver = webdriver.Chrome(options=options)
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        '''
    })
    
    return driver
```

---

## ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ

### ì „ììƒê±°ë˜ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘

```python
from src.scraper.advanced_scraper import AdvancedScraper

def scrape_ecommerce_site():
    scraper = AdvancedScraper()
    
    # 1. ì¹´í…Œê³ ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    categories = scraper.scrape_api('/api/categories')
    
    all_products = []
    
    for category in categories:
        # 2. ê° ì¹´í…Œê³ ë¦¬ì˜ ì œí’ˆ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜)
        products = scraper.scrape_with_pagination(
            f"/category/{category['id']}/products?page={{page}}",
            max_pages=10
        )
        
        for product in products:
            # 3. ì œí’ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            details = scraper.extract_hidden_data(product['url'])
            product.update(details)
            
            all_products.append(product)
        
        # Rate limiting
        time.sleep(1)
    
    return all_products
```

### ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘

```python
def scrape_news_site():
    scraper = AdvancedScraper()
    
    # ìµœì‹  ë‰´ìŠ¤ API
    latest_news = scraper.scrape_api('/api/news/latest')
    
    articles = []
    
    for news_item in latest_news:
        # ì „ì²´ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        article_html = scraper.session.get(news_item['url']).text
        soup = BeautifulSoup(article_html, 'html.parser')
        
        article = {
            'title': soup.find('h1').text,
            'content': soup.find('div', class_='content').text,
            'author': soup.find('span', class_='author').text,
            'date': soup.find('time')['datetime'],
            'tags': [tag.text for tag in soup.find_all('a', class_='tag')]
        }
        
        articles.append(article)
    
    return articles
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

| ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
|------|------|--------|
| 403 Forbidden | User-Agent ì°¨ë‹¨ | User-Agent ë¡œí…Œì´ì…˜ |
| 429 Too Many Requests | Rate limiting | ìš”ì²­ ê°„ê²© ëŠ˜ë¦¬ê¸° |
| CAPTCHA ì¶œí˜„ | ë´‡ ê°ì§€ | Selenium/Playwright ì‚¬ìš© |
| ë¹ˆ ì‘ë‹µ | JavaScript ë Œë”ë§ | ë™ì  ìŠ¤í¬ë˜í•‘ ë„êµ¬ ì‚¬ìš© |
| IP ì°¨ë‹¨ | ê³¼ë„í•œ ìš”ì²­ | í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ |
| ì„¸ì…˜ ë§Œë£Œ | ì¿ í‚¤ ë§Œë£Œ | ì„¸ì…˜ ê°±ì‹  ë¡œì§ êµ¬í˜„ |

### ë””ë²„ê¹… íŒ

```python
# ìš”ì²­/ì‘ë‹µ ë¡œê¹…
import logging

logging.basicConfig(level=logging.DEBUG)
logging.getLogger("requests").setLevel(logging.DEBUG)
logging.getLogger("urllib3").setLevel(logging.DEBUG)

# ì‘ë‹µ ì €ì¥
def save_response_for_debugging(response, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(response.text)
    
    print(f"Response saved to {filename}")
    print(f"Status Code: {response.status_code}")
    print(f"Headers: {response.headers}")
```

---

## ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ë‹¤ìŒì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤:
- âœ… ë‹¤ì–‘í•œ í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ ë°©ë²•
- âœ… ì¸ì¦ ë° ì„¸ì…˜ ê´€ë¦¬
- âœ… ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ
- âœ… Anti-Bot ë©”ì»¤ë‹ˆì¦˜ ìš°íšŒ
- âœ… ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘
- âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬

ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ìˆ ì„ ë§ˆìŠ¤í„°í•˜ì—¬ ì–´ë–¤ ì›¹ì‚¬ì´íŠ¸ë“  íš¨ê³¼ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€