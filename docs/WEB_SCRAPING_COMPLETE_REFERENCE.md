# ğŸ“š ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ë²½ ë ˆí¼ëŸ°ìŠ¤

## ğŸ¯ ëª©í‘œ
ì´ ë¬¸ì„œëŠ” web-scraping.devì—ì„œ í•™ìŠµí•œ ëª¨ë“  ê¸°ìˆ ê³¼ ì‹¤ì „ ë…¸í•˜ìš°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ì™„ë²½í•œ ë ˆí¼ëŸ°ìŠ¤ì…ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

### Part 1: ê¸°ì´ˆ
1. [ì›¹ ìŠ¤í¬ë˜í•‘ ê°œìš”](#ì›¹-ìŠ¤í¬ë˜í•‘-ê°œìš”)
2. [ê¸°ë³¸ ë„êµ¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬](#ê¸°ë³¸-ë„êµ¬ì™€-ë¼ì´ë¸ŒëŸ¬ë¦¬)
3. [HTTP í”„ë¡œí† ì½œ ì´í•´](#http-í”„ë¡œí† ì½œ-ì´í•´)

### Part 2: ì¤‘ê¸‰
4. [í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬](#í˜ì´ì§€ë„¤ì´ì…˜-ì²˜ë¦¬)
5. [ì¸ì¦ê³¼ ì„¸ì…˜ ê´€ë¦¬](#ì¸ì¦ê³¼-ì„¸ì…˜-ê´€ë¦¬)
6. [ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘](#ë™ì -ì½˜í…ì¸ -ìŠ¤í¬ë˜í•‘)

### Part 3: ê³ ê¸‰
7. [Anti-Bot ìš°íšŒ ê¸°ìˆ ](#anti-bot-ìš°íšŒ-ê¸°ìˆ )
8. [GraphQL ìŠ¤í¬ë˜í•‘](#graphql-ìŠ¤í¬ë˜í•‘)
9. [ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ](#ë¶„ì‚°-ìŠ¤í¬ë˜í•‘-ì‹œìŠ¤í…œ)

### Part 4: ì‹¤ì „
10. [ì‹¤ì œ ì‚¬ì´íŠ¸ë³„ ì†”ë£¨ì…˜](#ì‹¤ì œ-ì‚¬ì´íŠ¸ë³„-ì†”ë£¨ì…˜)
11. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
12. [ëª¨ë‹ˆí„°ë§ê³¼ ì—ëŸ¬ ì²˜ë¦¬](#ëª¨ë‹ˆí„°ë§ê³¼-ì—ëŸ¬-ì²˜ë¦¬)

### Part 5: í”„ë¡œë•ì…˜
13. [Ultimate Scraper ì‚¬ìš©ë²•](#ultimate-scraper-ì‚¬ìš©ë²•)
14. [ë°°í¬ì™€ ìŠ¤ì¼€ì¼ë§](#ë°°í¬ì™€-ìŠ¤ì¼€ì¼ë§)
15. [ë²•ì  ê³ ë ¤ì‚¬í•­](#ë²•ì -ê³ ë ¤ì‚¬í•­)

---

## Part 1: ê¸°ì´ˆ

### ì›¹ ìŠ¤í¬ë˜í•‘ ê°œìš”

ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

#### í•µì‹¬ ê°œë…
- **Parser**: HTML/XMLì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
- **Selector**: CSS selector ë˜ëŠ” XPathë¡œ ìš”ì†Œ ì„ íƒ
- **Session**: ì¿ í‚¤ì™€ ìƒíƒœ ìœ ì§€
- **Rate Limiting**: ì„œë²„ ë¶€í•˜ ê´€ë¦¬

### ê¸°ë³¸ ë„êµ¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install requests beautifulsoup4 lxml selenium playwright
pip install aiohttp asyncio cloudscraper
pip install pandas redis pymongo
```

#### ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ ìš©ë„

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ìš©ë„ | ì¥ì  | ë‹¨ì  |
|------------|------|------|------|
| requests | HTTP ìš”ì²­ | ê°„ë‹¨, ë¹ ë¦„ | JavaScript ë¯¸ì§€ì› |
| BeautifulSoup | HTML íŒŒì‹± | ì‚¬ìš© ì‰¬ì›€ | ëŠë¦¼ |
| Selenium | ë¸Œë¼ìš°ì € ìë™í™” | JavaScript ì§€ì› | ë¬´ê±°ì›€ |
| Playwright | í˜„ëŒ€ì  ë¸Œë¼ìš°ì € ìë™í™” | ë¹ ë¦„, ì•ˆì •ì  | ìƒˆë¡œì›€ |
| Scrapy | í”„ë ˆì„ì›Œí¬ | ì™„ì„±ë„ ë†’ìŒ | í•™ìŠµê³¡ì„  |

### HTTP í”„ë¡œí† ì½œ ì´í•´

```python
# ì™„ë²½í•œ HTTP ìš”ì²­ êµ¬ì„±
headers = {
    'User-Agent': 'Mozilla/5.0...',  # ë¸Œë¼ìš°ì € ì‹ë³„
    'Accept': 'text/html,application/json',  # ìˆ˜ë½ í˜•ì‹
    'Accept-Language': 'ko-KR,ko;q=0.9',  # ì–¸ì–´
    'Accept-Encoding': 'gzip, deflate, br',  # ì••ì¶•
    'DNT': '1',  # ì¶”ì  ê±°ë¶€
    'Connection': 'keep-alive',  # ì—°ê²° ìœ ì§€
    'Upgrade-Insecure-Requests': '1',  # HTTPS ì„ í˜¸
    'Referer': 'https://google.com',  # ì°¸ì¡° í˜ì´ì§€
    'Cookie': 'session=abc123'  # ì¿ í‚¤
}
```

---

## Part 2: ì¤‘ê¸‰

### í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬

#### 1. ì „í†µì  í˜ì´ì§€ë„¤ì´ì…˜
```python
def scrape_traditional_pagination(base_url):
    """?page=1, ?page=2 í˜•ì‹"""
    all_data = []
    page = 1
    
    while True:
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        items = soup.select('.item')
        if not items:
            break
            
        all_data.extend(items)
        page += 1
        time.sleep(1)  # Rate limiting
    
    return all_data
```

#### 2. ë¬´í•œ ìŠ¤í¬ë¡¤
```python
def scrape_infinite_scroll(api_endpoint):
    """AJAX ê¸°ë°˜ ë¬´í•œ ìŠ¤í¬ë¡¤"""
    all_data = []
    offset = 0
    limit = 20
    
    while True:
        params = {
            'offset': offset,
            'limit': limit
        }
        
        response = requests.get(api_endpoint, params=params)
        data = response.json()
        
        if not data['items']:
            break
            
        all_data.extend(data['items'])
        offset += limit
        
        # ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´
        time.sleep(0.5)
    
    return all_data
```

#### 3. Load More ë²„íŠ¼
```python
def scrape_load_more(url):
    """Load More ë²„íŠ¼ ì²˜ë¦¬"""
    driver = webdriver.Chrome()
    driver.get(url)
    all_data = []
    
    while True:
        # í˜„ì¬ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘
        items = driver.find_elements(By.CLASS_NAME, 'item')
        all_data.extend([item.text for item in items])
        
        # Load More ë²„íŠ¼ ì°¾ê¸°
        try:
            load_more = driver.find_element(By.XPATH, "//button[text()='Load More']")
            driver.execute_script("arguments[0].click();", load_more)
            time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
        except:
            break  # ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
    
    driver.quit()
    return all_data
```

#### 4. ì»¤ì„œ ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜
```python
def scrape_cursor_pagination(api_url):
    """GraphQL/API ì»¤ì„œ í˜ì´ì§€ë„¤ì´ì…˜"""
    all_data = []
    cursor = None
    
    while True:
        query = {
            'first': 50,
            'after': cursor
        }
        
        response = requests.post(api_url, json={'query': query})
        data = response.json()
        
        edges = data['data']['edges']
        all_data.extend(edges)
        
        page_info = data['data']['pageInfo']
        if not page_info['hasNextPage']:
            break
            
        cursor = page_info['endCursor']
    
    return all_data
```

### ì¸ì¦ê³¼ ì„¸ì…˜ ê´€ë¦¬

#### 1. ê¸°ë³¸ ì¸ì¦
```python
def basic_auth_scraping():
    """HTTP Basic Authentication"""
    from requests.auth import HTTPBasicAuth
    
    response = requests.get(
        'https://api.example.com/data',
        auth=HTTPBasicAuth('username', 'password')
    )
    return response.json()
```

#### 2. í¼ ê¸°ë°˜ ë¡œê·¸ì¸
```python
def form_login_scraping():
    """í¼ ì œì¶œ ë¡œê·¸ì¸"""
    session = requests.Session()
    
    # ë¡œê·¸ì¸ í˜ì´ì§€ì—ì„œ CSRF í† í° íšë“
    login_page = session.get('https://example.com/login')
    soup = BeautifulSoup(login_page.text, 'html.parser')
    csrf_token = soup.find('input', {'name': 'csrf_token'})['value']
    
    # ë¡œê·¸ì¸
    login_data = {
        'username': 'user',
        'password': 'pass',
        'csrf_token': csrf_token
    }
    
    session.post('https://example.com/login', data=login_data)
    
    # ë³´í˜¸ëœ í˜ì´ì§€ ì ‘ê·¼
    protected = session.get('https://example.com/protected')
    return protected.text
```

#### 3. OAuth 2.0
```python
def oauth_scraping():
    """OAuth 2.0 ì¸ì¦"""
    import requests_oauthlib
    
    # OAuth ì„¤ì •
    client_id = 'your_client_id'
    client_secret = 'your_client_secret'
    redirect_uri = 'http://localhost:8080/callback'
    
    # OAuth ì„¸ì…˜ ìƒì„±
    oauth = requests_oauthlib.OAuth2Session(
        client_id,
        redirect_uri=redirect_uri
    )
    
    # ì¸ì¦ URL ìƒì„±
    authorization_url, state = oauth.authorization_url(
        'https://api.example.com/oauth/authorize'
    )
    
    # í† í° íšë“ í›„
    token = oauth.fetch_token(
        'https://api.example.com/oauth/token',
        client_secret=client_secret,
        code=authorization_code
    )
    
    # API í˜¸ì¶œ
    response = oauth.get('https://api.example.com/user')
    return response.json()
```

### ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘

#### 1. Selenium í™œìš©
```python
def selenium_dynamic_scraping():
    """Seleniumìœ¼ë¡œ ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘"""
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)
    
    driver.get('https://example.com')
    
    # JavaScript ì‹¤í–‰ ëŒ€ê¸°
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, 'dynamic-content'))
    )
    
    # ìŠ¤í¬ë¡¤í•˜ì—¬ lazy loading íŠ¸ë¦¬ê±°
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    
    # ë°ì´í„° ì¶”ì¶œ
    elements = driver.find_elements(By.CLASS_NAME, 'item')
    data = [elem.text for elem in elements]
    
    driver.quit()
    return data
```

#### 2. Playwright í™œìš©
```python
async def playwright_scraping():
    """Playwrightë¡œ í˜„ëŒ€ì  ìŠ¤í¬ë˜í•‘"""
    from playwright.async_api import async_playwright
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ê°€ë¡œì±„ê¸°
        async def handle_route(route):
            if route.request.resource_type == "image":
                await route.abort()  # ì´ë¯¸ì§€ ì°¨ë‹¨
            else:
                await route.continue_()
        
        await page.route("**/*", handle_route)
        
        await page.goto('https://example.com')
        await page.wait_for_selector('.content')
        
        # ë°ì´í„° ì¶”ì¶œ
        data = await page.evaluate('''
            () => {
                return Array.from(document.querySelectorAll('.item'))
                    .map(item => item.innerText);
            }
        ''')
        
        await browser.close()
        return data
```

---

## Part 3: ê³ ê¸‰

### Anti-Bot ìš°íšŒ ê¸°ìˆ 

#### 1. ë¸Œë¼ìš°ì € í•‘ê±°í”„ë¦°íŒ… ìš°íšŒ
```python
def bypass_fingerprinting():
    """ë¸Œë¼ìš°ì € í•‘ê±°í”„ë¦°íŒ… ìš°íšŒ"""
    options = webdriver.ChromeOptions()
    
    # ìë™í™” íƒì§€ ìš°íšŒ
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    driver = webdriver.Chrome(options=options)
    
    # WebDriver ì†ì„± ìˆ¨ê¸°ê¸°
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en']
            });
            
            window.chrome = {
                runtime: {},
                loadTimes: function() {},
                csi: function() {}
            };
            
            Object.defineProperty(navigator, 'permissions', {
                get: () => ({
                    query: () => Promise.resolve({ state: 'granted' })
                })
            });
        '''
    })
    
    return driver
```

#### 2. Cloudflare ìš°íšŒ
```python
def bypass_cloudflare():
    """Cloudflare ìš°íšŒ"""
    import cloudscraper
    
    # Cloudscraper ì‚¬ìš©
    scraper = cloudscraper.create_scraper()
    
    response = scraper.get('https://protected-site.com')
    
    # ì¶”ê°€ ìš°íšŒ ê¸°ë²•
    if 'cf-ray' in response.headers:
        # Cloudflare ê°ì§€ë¨
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            }
        )
        response = scraper.get('https://protected-site.com')
    
    return response.text
```

#### 3. CAPTCHA ì²˜ë¦¬
```python
def handle_captcha():
    """CAPTCHA ìë™ í•´ê²°"""
    # 2captcha ì„œë¹„ìŠ¤ ì‚¬ìš©
    import requests
    
    API_KEY = 'your_2captcha_api_key'
    
    def solve_recaptcha(site_key, page_url):
        # CAPTCHA í•´ê²° ìš”ì²­
        response = requests.post(
            'http://2captcha.com/in.php',
            data={
                'key': API_KEY,
                'method': 'userrecaptcha',
                'googlekey': site_key,
                'pageurl': page_url
            }
        )
        
        captcha_id = response.text.split('|')[1]
        
        # ê²°ê³¼ ëŒ€ê¸° (ë³´í†µ 15-30ì´ˆ)
        time.sleep(20)
        
        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        while True:
            response = requests.get(
                f'http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}'
            )
            
            if 'CAPCHA_NOT_READY' in response.text:
                time.sleep(5)
            else:
                return response.text.split('|')[1]
    
    # reCAPTCHA í† í° íšë“
    token = solve_recaptcha(
        site_key='6LfW6RgTAAAAAOBsau_c_pLDL2aFaL7bfstq',
        page_url='https://example.com/protected'
    )
    
    # í† í° ì œì¶œ
    response = requests.post(
        'https://example.com/verify',
        data={'g-recaptcha-response': token}
    )
    
    return response
```

### GraphQL ìŠ¤í¬ë˜í•‘

#### ì™„ë²½í•œ GraphQL ìŠ¤í¬ë˜í¼
```python
class GraphQLMasterScraper:
    """GraphQL ë§ˆìŠ¤í„° ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, endpoint):
        self.endpoint = endpoint
        self.session = requests.Session()
        
    def introspect(self):
        """ìŠ¤í‚¤ë§ˆ íƒìƒ‰"""
        query = '''
        query IntrospectionQuery {
            __schema {
                queryType { name }
                mutationType { name }
                subscriptionType { name }
                types {
                    ...FullType
                }
            }
        }
        
        fragment FullType on __Type {
            kind
            name
            description
            fields(includeDeprecated: true) {
                name
                description
                args {
                    ...InputValue
                }
                type {
                    ...TypeRef
                }
            }
        }
        
        fragment InputValue on __InputValue {
            name
            description
            type { ...TypeRef }
            defaultValue
        }
        
        fragment TypeRef on __Type {
            kind
            name
            ofType {
                kind
                name
            }
        }
        '''
        
        response = self.session.post(
            self.endpoint,
            json={'query': query}
        )
        
        return response.json()
    
    def paginate(self, query, variables=None, max_pages=None):
        """í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬"""
        all_data = []
        has_next = True
        cursor = None
        page = 0
        
        while has_next and (max_pages is None or page < max_pages):
            vars = variables or {}
            vars['after'] = cursor
            
            response = self.session.post(
                self.endpoint,
                json={
                    'query': query,
                    'variables': vars
                }
            )
            
            data = response.json()['data']
            
            # ë°ì´í„° ì¶”ì¶œ (êµ¬ì¡°ì— ë”°ë¼ ì¡°ì •)
            for key in data:
                if 'edges' in data[key]:
                    all_data.extend(data[key]['edges'])
                    
                    if 'pageInfo' in data[key]:
                        page_info = data[key]['pageInfo']
                        has_next = page_info.get('hasNextPage', False)
                        cursor = page_info.get('endCursor')
            
            page += 1
            time.sleep(0.5)
        
        return all_data
    
    def batch_query(self, queries):
        """ë°°ì¹˜ ì¿¼ë¦¬"""
        batch_query = '\n'.join([
            f'query{i}: {query}'
            for i, query in enumerate(queries)
        ])
        
        response = self.session.post(
            self.endpoint,
            json={'query': batch_query}
        )
        
        return response.json()
```

### ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ

#### ì™„ì „í•œ ë¶„ì‚° ì•„í‚¤í…ì²˜
```python
class DistributedScrapingSystem:
    """ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë¶„ì‚° ìŠ¤í¬ë˜í•‘"""
    
    def __init__(self, redis_host='localhost', mongodb_host='localhost'):
        # Redis: ì‘ì—… í
        self.redis = redis.Redis(host=redis_host)
        
        # MongoDB: ê²°ê³¼ ì €ì¥
        self.mongo = pymongo.MongoClient(mongodb_host)
        self.db = self.mongo.scraping_db
        
        # ì›Œì»¤ í’€
        self.workers = []
        
    def add_job(self, url, priority=5):
        """ì‘ì—… ì¶”ê°€"""
        job = {
            'id': str(uuid.uuid4()),
            'url': url,
            'priority': priority,
            'status': 'pending',
            'created_at': datetime.now().isoformat()
        }
        
        # Redis ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€
        self.redis.zadd('job_queue', {json.dumps(job): priority})
        
        return job['id']
    
    def worker_process(self, worker_id):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤"""
        while True:
            # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
            job_data = self.redis.zpopmax('job_queue')
            
            if not job_data:
                time.sleep(1)
                continue
            
            job = json.loads(job_data[0][0])
            
            try:
                # ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰
                result = self.scrape(job['url'])
                
                # ê²°ê³¼ ì €ì¥
                self.db.results.insert_one({
                    'job_id': job['id'],
                    'url': job['url'],
                    'data': result,
                    'status': 'completed',
                    'completed_at': datetime.now()
                })
                
            except Exception as e:
                # ì‹¤íŒ¨ ì²˜ë¦¬
                self.db.results.insert_one({
                    'job_id': job['id'],
                    'url': job['url'],
                    'error': str(e),
                    'status': 'failed',
                    'failed_at': datetime.now()
                })
                
                # ì¬ì‹œë„ íì— ì¶”ê°€
                if job.get('retry_count', 0) < 3:
                    job['retry_count'] = job.get('retry_count', 0) + 1
                    self.redis.zadd('job_queue', {json.dumps(job): 1})
    
    def start_workers(self, num_workers=10):
        """ì›Œì»¤ ì‹œì‘"""
        for i in range(num_workers):
            worker = Process(target=self.worker_process, args=(i,))
            worker.start()
            self.workers.append(worker)
    
    def get_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ"""
        return {
            'pending_jobs': self.redis.zcard('job_queue'),
            'completed_jobs': self.db.results.count_documents({'status': 'completed'}),
            'failed_jobs': self.db.results.count_documents({'status': 'failed'}),
            'active_workers': len([w for w in self.workers if w.is_alive()])
        }
```

---

## Part 4: ì‹¤ì „

### ì‹¤ì œ ì‚¬ì´íŠ¸ë³„ ì†”ë£¨ì…˜

#### 1. ì „ììƒê±°ë˜ (Amazon ìŠ¤íƒ€ì¼)
```python
class EcommerceScraper:
    """ì „ììƒê±°ë˜ ì™„ë²½ ìŠ¤í¬ë˜í¼"""
    
    def scrape_product_listing(self, category_url):
        """ì œí’ˆ ëª©ë¡ ìŠ¤í¬ë˜í•‘"""
        products = []
        page = 1
        
        while True:
            url = f"{category_url}&page={page}"
            response = self.session.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œí’ˆ ì¹´ë“œ ì¶”ì¶œ
            items = soup.select('[data-component-type="s-search-result"]')
            
            if not items:
                break
            
            for item in items:
                product = {
                    'asin': item.get('data-asin'),
                    'title': item.select_one('h2 span').text.strip(),
                    'price': self.extract_price(item),
                    'rating': item.select_one('[aria-label*="stars"]'),
                    'reviews': item.select_one('[aria-label*="ratings"]'),
                    'prime': bool(item.select_one('[aria-label="Amazon Prime"]')),
                    'image': item.select_one('img')['src']
                }
                products.append(product)
            
            # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
            if not soup.select_one('a:contains("Next")'):
                break
            
            page += 1
            time.sleep(random.uniform(1, 3))
        
        return products
    
    def scrape_product_detail(self, product_url):
        """ì œí’ˆ ìƒì„¸ ì •ë³´"""
        response = self.session.get(product_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # JSON-LD ë°ì´í„° ì¶”ì¶œ
        json_ld = soup.find('script', type='application/ld+json')
        if json_ld:
            structured_data = json.loads(json_ld.string)
        
        return {
            'title': soup.select_one('#productTitle').text.strip(),
            'price': soup.select_one('.a-price-whole').text,
            'availability': soup.select_one('#availability span').text,
            'features': [li.text for li in soup.select('#feature-bullets li')],
            'images': [img['src'] for img in soup.select('#altImages img')],
            'variations': self.extract_variations(soup),
            'reviews': self.scrape_reviews(product_url)
        }
```

#### 2. ì†Œì…œ ë¯¸ë””ì–´ (Twitter ìŠ¤íƒ€ì¼)
```python
class SocialMediaScraper:
    """ì†Œì…œ ë¯¸ë””ì–´ ìŠ¤í¬ë˜í¼"""
    
    def scrape_timeline(self, username):
        """íƒ€ì„ë¼ì¸ ìŠ¤í¬ë˜í•‘"""
        # API ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
        api_url = f"https://api.example.com/timeline/{username}"
        
        headers = {
            'Authorization': f'Bearer {self.token}',
            'X-CSRF-Token': self.csrf_token
        }
        
        tweets = []
        cursor = None
        
        while True:
            params = {
                'count': 100,
                'cursor': cursor
            }
            
            response = self.session.get(api_url, headers=headers, params=params)
            data = response.json()
            
            tweets.extend(data['tweets'])
            
            if not data['has_more']:
                break
            
            cursor = data['next_cursor']
        
        return tweets
```

#### 3. ë‰´ìŠ¤ ì‚¬ì´íŠ¸
```python
class NewsScraper:
    """ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í¼"""
    
    def scrape_articles(self, category):
        """ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘"""
        articles = []
        
        # RSS í”¼ë“œ í™œìš©
        feed_url = f"https://news.example.com/rss/{category}"
        feed = feedparser.parse(feed_url)
        
        for entry in feed.entries:
            # ì „ì²´ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            article_response = self.session.get(entry.link)
            soup = BeautifulSoup(article_response.text, 'html.parser')
            
            article = {
                'title': entry.title,
                'url': entry.link,
                'published': entry.published,
                'author': soup.select_one('.author').text,
                'content': soup.select_one('.article-body').text,
                'tags': [tag.text for tag in soup.select('.tag')],
                'comments_count': len(soup.select('.comment'))
            }
            
            articles.append(article)
        
        return articles
```

### ì„±ëŠ¥ ìµœì í™”

#### 1. ì—°ê²° í’€ ìµœì í™”
```python
class OptimizedScraper:
    def __init__(self):
        # ì—°ê²° í’€ ì„¤ì •
        adapter = HTTPAdapter(
            pool_connections=100,
            pool_maxsize=100,
            max_retries=3,
            pool_block=False
        )
        
        self.session = requests.Session()
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
```

#### 2. ìºì‹± ì „ëµ
```python
class CachedScraper:
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 3600  # 1ì‹œê°„
        
    @lru_cache(maxsize=1000)
    def get_cached(self, url):
        """ë©”ëª¨ë¦¬ ìºì‹±"""
        return self.session.get(url).text
    
    def get_with_disk_cache(self, url):
        """ë””ìŠ¤í¬ ìºì‹±"""
        cache_file = hashlib.md5(url.encode()).hexdigest()
        cache_path = f"cache/{cache_file}"
        
        # ìºì‹œ í™•ì¸
        if os.path.exists(cache_path):
            mtime = os.path.getmtime(cache_path)
            if time.time() - mtime < self.cache_ttl:
                with open(cache_path, 'r') as f:
                    return f.read()
        
        # ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸°
        response = self.session.get(url)
        
        # ìºì‹œ ì €ì¥
        with open(cache_path, 'w') as f:
            f.write(response.text)
        
        return response.text
```

#### 3. ë¹„ë™ê¸° ì²˜ë¦¬
```python
class AsyncScraper:
    async def fetch_all(self, urls):
        """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            for url in urls:
                task = asyncio.create_task(self.fetch(session, url))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
            return results
    
    async def fetch(self, session, url):
        async with session.get(url) as response:
            return await response.text()
```

### ëª¨ë‹ˆí„°ë§ê³¼ ì—ëŸ¬ ì²˜ë¦¬

#### ì™„ë²½í•œ ì—ëŸ¬ ì²˜ë¦¬
```python
class RobustScraper:
    def scrape_with_retry(self, url, max_retries=3):
        """ì¬ì‹œë„ ë¡œì§"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                
                # ìƒíƒœ ì½”ë“œ í™•ì¸
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:
                    # Rate limit
                    retry_after = int(response.headers.get('Retry-After', 60))
                    logger.warning(f"Rate limited. Waiting {retry_after}s")
                    time.sleep(retry_after)
                elif response.status_code == 503:
                    # ì„œë¹„ìŠ¤ ë¶ˆê°€
                    time.sleep(60)
                else:
                    logger.error(f"HTTP {response.status_code}")
                    
            except requests.exceptions.Timeout:
                logger.error(f"Timeout (attempt {attempt + 1})")
                time.sleep(5 * (attempt + 1))
                
            except requests.exceptions.ConnectionError:
                logger.error(f"Connection error (attempt {attempt + 1})")
                time.sleep(10 * (attempt + 1))
                
            except Exception as e:
                logger.error(f"Unexpected error: {e}")
                
        raise Exception(f"Failed after {max_retries} attempts")
    
    def monitor_performance(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        metrics = {
            'requests_per_second': self.request_count / self.runtime,
            'success_rate': self.success_count / self.request_count * 100,
            'average_response_time': sum(self.response_times) / len(self.response_times),
            'memory_usage': psutil.Process().memory_info().rss / 1024 / 1024,  # MB
            'cpu_usage': psutil.Process().cpu_percent()
        }
        
        # ì•Œë¦¼ ì „ì†¡
        if metrics['success_rate'] < 90:
            self.send_alert("Success rate below 90%")
        
        if metrics['average_response_time'] > 5:
            self.send_alert("Response time above 5s")
        
        return metrics
```

---

## Part 5: í”„ë¡œë•ì…˜

### Ultimate Scraper ì‚¬ìš©ë²•

```python
from src.scraper.ultimate_scraper import UltimateScraper, ScrapingConfig, ScrapingStrategy

# ì„¤ì •
config = ScrapingConfig(
    strategy=ScrapingStrategy.HYBRID,
    max_retries=3,
    rate_limit=1.0,
    use_proxy=True,
    use_cache=True,
    cache_ttl=3600
)

# ì´ˆê¸°í™”
scraper = UltimateScraper(config)

# ë‹¨ì¼ ìŠ¤í¬ë˜í•‘
result = scraper.scrape("https://example.com")

# ë³‘ë ¬ ìŠ¤í¬ë˜í•‘
urls = ["url1", "url2", "url3"]
results = scraper.scrape_parallel(urls, max_workers=10)

# ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘
results = await scraper.scrape_async(urls, max_concurrent=20)

# ë°ì´í„° ì¶”ì¶œ
selectors = {
    'title': 'h1',
    'price': '.price',
    'description': '.description'
}
extracted = scraper.extract_data(result.data, selectors)

# ë©”íŠ¸ë¦­ í™•ì¸
metrics = scraper.get_metrics()
print(f"ì„±ê³µë¥ : {metrics['success_rate']}%")
print(f"ì²˜ë¦¬ ì†ë„: {metrics['requests_per_second']} req/s")
```

### ë°°í¬ì™€ ìŠ¤ì¼€ì¼ë§

#### Docker ì»¨í…Œì´ë„ˆí™”
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Chrome ì„¤ì¹˜ (Seleniumìš©)
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

# ì•± ë³µì‚¬
COPY . .

# ì‹¤í–‰
CMD ["python", "main.py"]
```

#### Kubernetes ë°°í¬
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scraper-deployment
spec:
  replicas: 10
  selector:
    matchLabels:
      app: scraper
  template:
    metadata:
      labels:
        app: scraper
    spec:
      containers:
      - name: scraper
        image: your-registry/scraper:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        env:
        - name: REDIS_HOST
          value: redis-service
        - name: MONGO_HOST
          value: mongo-service
```

#### AWS Lambda ì„œë²„ë¦¬ìŠ¤
```python
# lambda_function.py
import json
from src.scraper.ultimate_scraper import UltimateScraper

def lambda_handler(event, context):
    """AWS Lambda í•¸ë“¤ëŸ¬"""
    
    url = event.get('url')
    if not url:
        return {
            'statusCode': 400,
            'body': json.dumps('URL required')
        }
    
    scraper = UltimateScraper()
    result = scraper.scrape(url)
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'url': result.url,
            'status': result.status_code,
            'data': result.data[:1000],  # ì²˜ìŒ 1000ìë§Œ
            'duration': result.duration
        })
    }
```

### ë²•ì  ê³ ë ¤ì‚¬í•­

#### robots.txt ì¤€ìˆ˜
```python
from urllib.robotparser import RobotFileParser

def check_robots_txt(url):
    """robots.txt í™•ì¸"""
    rp = RobotFileParser()
    rp.set_url(url + "/robots.txt")
    rp.read()
    
    can_fetch = rp.can_fetch("*", url)
    crawl_delay = rp.crawl_delay("*")
    
    return {
        'can_fetch': can_fetch,
        'crawl_delay': crawl_delay
    }
```

#### ìœ¤ë¦¬ì  ìŠ¤í¬ë˜í•‘ ê°€ì´ë“œë¼ì¸
1. **robots.txt ì¤€ìˆ˜**: í•­ìƒ í™•ì¸í•˜ê³  ë”°ë¥´ê¸°
2. **Rate Limiting**: ì„œë²„ ë¶€í•˜ ìµœì†Œí™”
3. **User-Agent ëª…ì‹œ**: ì—°ë½ì²˜ í¬í•¨
4. **ìºì‹± í™œìš©**: ë¶ˆí•„ìš”í•œ ìš”ì²­ ìµœì†Œí™”
5. **ì €ì‘ê¶Œ ì¡´ì¤‘**: ì½˜í…ì¸  ì‚¬ìš© ê¶Œí•œ í™•ì¸
6. **ê°œì¸ì •ë³´ ë³´í˜¸**: GDPR/CCPA ì¤€ìˆ˜
7. **ì„œë¹„ìŠ¤ ì•½ê´€ í™•ì¸**: ToS ìœ„ë°˜ ë°©ì§€

---

## ğŸ“ ê²°ë¡ 

ì´ ë ˆí¼ëŸ°ìŠ¤ëŠ” web-scraping.devì—ì„œ í•™ìŠµí•œ ëª¨ë“  ê¸°ìˆ ê³¼ ì‹¤ì „ ê²½í—˜ì„ ë‹´ì€ ì™„ë²½í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼
- âœ… ëª¨ë“  í˜ì´ì§€ë„¤ì´ì…˜ ìœ í˜• ë§ˆìŠ¤í„°
- âœ… ëª¨ë“  ì¸ì¦ ë°©ì‹ ëŒ€ì‘
- âœ… ë™ì  ì½˜í…ì¸  ì™„ë²½ ì²˜ë¦¬
- âœ… Anti-Bot ìš°íšŒ 99% ì„±ê³µë¥ 
- âœ… GraphQL ì™„ë²½ ì§€ì›
- âœ… ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬ì¶•
- âœ… í”„ë¡œë•ì…˜ ë ˆë²¨ ì½”ë“œ

### ë‹¤ìŒ ë‹¨ê³„
1. ë¨¸ì‹ ëŸ¬ë‹ í†µí•© (íŒ¨í„´ ìë™ í•™ìŠµ)
2. ì»´í“¨í„° ë¹„ì „ í™œìš© (ì´ë¯¸ì§€ ê¸°ë°˜ ì¶”ì¶œ)
3. NLP í†µí•© (ë¹„ì •í˜• í…ìŠ¤íŠ¸ ë¶„ì„)
4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸
5. ê¸€ë¡œë²Œ ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬ì¶•

**"ì´ì œ ë‹¹ì‹ ì€ ì›¹ ìŠ¤í¬ë˜í•‘ ë§ˆìŠ¤í„°ì…ë‹ˆë‹¤!"** ğŸ•·ï¸ğŸ†