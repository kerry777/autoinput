# -*- coding: utf-8 -*-
"""
web-scraping.dev ê³ ê¸‰ ê¸°ëŠ¥ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import requests
from bs4 import BeautifulSoup
import json
from typing import Dict, List, Any
import time

class WebScrapingDevExplorer:
    """web-scraping.dev ì‚¬ì´íŠ¸ íƒìƒ‰ ë° í•™ìŠµ"""
    
    def __init__(self):
        self.base_url = "https://web-scraping.dev"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def explore_challenges(self) -> Dict[str, List[str]]:
        """ëª¨ë“  ì±Œë¦°ì§€ ì¹´í…Œê³ ë¦¬ì™€ ë§í¬ ìˆ˜ì§‘"""
        print("\nğŸ” web-scraping.dev ì±Œë¦°ì§€ íƒìƒ‰ ì¤‘...")
        
        response = self.session.get(self.base_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        challenges = {
            'paging': [],
            'authentication': [],
            'data_extraction': [],
            'anti_scraping': [],
            'dynamic_content': []
        }
        
        # ëª¨ë“  ì±Œë¦°ì§€ ë§í¬ ì°¾ê¸°
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            if 'page' in href.lower() or 'paging' in href.lower():
                challenges['paging'].append({
                    'url': self.base_url + href if not href.startswith('http') else href,
                    'title': text
                })
            elif 'login' in href.lower() or 'auth' in href.lower() or 'csrf' in href.lower():
                challenges['authentication'].append({
                    'url': self.base_url + href if not href.startswith('http') else href,
                    'title': text
                })
            elif 'block' in href.lower() or 'redirect' in href.lower():
                challenges['anti_scraping'].append({
                    'url': self.base_url + href if not href.startswith('http') else href,
                    'title': text
                })
            elif 'json' in href.lower() or 'pdf' in href.lower() or 'storage' in href.lower():
                challenges['data_extraction'].append({
                    'url': self.base_url + href if not href.startswith('http') else href,
                    'title': text
                })
            elif 'scroll' in href.lower() or 'load' in href.lower() or 'graphql' in href.lower():
                challenges['dynamic_content'].append({
                    'url': self.base_url + href if not href.startswith('http') else href,
                    'title': text
                })
        
        return challenges
    
    def test_product_api(self) -> Dict[str, Any]:
        """Product API í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”Œ Product API í…ŒìŠ¤íŠ¸...")
        
        # ì œí’ˆ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        api_url = f"{self.base_url}/api/products"
        response = self.session.get(api_url)
        
        if response.status_code == 200:
            products = response.json()
            print(f"âœ… {len(products)} ê°œ ì œí’ˆ ë°œê²¬")
            
            # ì²« ë²ˆì§¸ ì œí’ˆ ìƒì„¸ ì •ë³´
            if products:
                first_product = products[0]
                detail_url = f"{self.base_url}/api/products/{first_product.get('id', 1)}"
                detail_response = self.session.get(detail_url)
                
                if detail_response.status_code == 200:
                    return detail_response.json()
        
        return {}
    
    def test_pagination(self) -> List[Dict]:
        """í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸...")
        
        all_products = []
        page = 1
        max_pages = 3  # ë°ëª¨ìš©ìœ¼ë¡œ 3í˜ì´ì§€ë§Œ
        
        while page <= max_pages:
            url = f"{self.base_url}/products?page={page}"
            response = self.session.get(url)
            
            if response.status_code != 200:
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            products = soup.find_all('div', class_='product')
            
            if not products:
                break
            
            print(f"  í˜ì´ì§€ {page}: {len(products)}ê°œ ì œí’ˆ")
            
            for product in products:
                title = product.find('h3')
                price = product.find('span', class_='price')
                
                if title and price:
                    all_products.append({
                        'title': title.text.strip(),
                        'price': price.text.strip()
                    })
            
            page += 1
            time.sleep(0.5)  # Rate limiting
        
        return all_products
    
    def test_hidden_data(self) -> Dict:
        """ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
        
        url = f"{self.base_url}/products/1"
        response = self.session.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        hidden_data = {}
        
        # JavaScript ë³€ìˆ˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'window.' in script.string:
                # window.productData ê°™ì€ íŒ¨í„´ ì°¾ê¸°
                lines = script.string.split('\n')
                for line in lines:
                    if 'window.' in line and '=' in line:
                        try:
                            # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• í•„ìš”)
                            var_name = line.split('window.')[1].split('=')[0].strip()
                            var_value = line.split('=')[1].strip().rstrip(';')
                            
                            # JSON íŒŒì‹± ì‹œë„
                            if var_value.startswith('{') or var_value.startswith('['):
                                try:
                                    hidden_data[var_name] = json.loads(var_value)
                                except:
                                    hidden_data[var_name] = var_value
                        except:
                            pass
        
        # data ì†ì„±ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        elements_with_data = soup.find_all(attrs={"data-product-id": True})
        for elem in elements_with_data:
            hidden_data['product_id'] = elem.get('data-product-id')
        
        return hidden_data
    
    def generate_report(self, challenges: Dict, api_data: Dict, products: List, hidden: Dict):
        """í•™ìŠµ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 80)
        print("ğŸ“Š web-scraping.dev í•™ìŠµ ë¦¬í¬íŠ¸")
        print("=" * 80)
        
        # ì±Œë¦°ì§€ ìš”ì•½
        print("\nğŸ¯ ë°œê²¬ëœ ì±Œë¦°ì§€:")
        for category, items in challenges.items():
            if items:
                print(f"\n  {category.upper()} ({len(items)}ê°œ):")
                for item in items[:3]:  # ê° ì¹´í…Œê³ ë¦¬ë³„ 3ê°œë§Œ í‘œì‹œ
                    print(f"    â€¢ {item['title']}")
        
        # API ë°ì´í„°
        if api_data:
            print(f"\nğŸ”Œ API ë°ì´í„° ì˜ˆì‹œ:")
            print(f"  ì œí’ˆëª…: {api_data.get('name', 'N/A')}")
            print(f"  ê°€ê²©: ${api_data.get('price', 'N/A')}")
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ê²°ê³¼
        if products:
            print(f"\nğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ ê²°ê³¼:")
            print(f"  ì´ {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘")
            if products:
                print(f"  ì²« ì œí’ˆ: {products[0]['title']} - {products[0]['price']}")
        
        # ìˆ¨ê²¨ì§„ ë°ì´í„°
        if hidden:
            print(f"\nğŸ” ìˆ¨ê²¨ì§„ ë°ì´í„° ë°œê²¬:")
            for key, value in list(hidden.items())[:3]:
                print(f"  {key}: {str(value)[:50]}...")
        
        print("\n" + "=" * 80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    explorer = WebScrapingDevExplorer()
    
    # 1. ì±Œë¦°ì§€ íƒìƒ‰
    challenges = explorer.explore_challenges()
    
    # 2. API í…ŒìŠ¤íŠ¸
    api_data = explorer.test_product_api()
    
    # 3. í˜ì´ì§€ë„¤ì´ì…˜ í…ŒìŠ¤íŠ¸
    products = explorer.test_pagination()
    
    # 4. ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ
    hidden_data = explorer.test_hidden_data()
    
    # 5. ë¦¬í¬íŠ¸ ìƒì„±
    explorer.generate_report(challenges, api_data, products, hidden_data)
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„ë¡œ ëª¨ë“ˆí™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()