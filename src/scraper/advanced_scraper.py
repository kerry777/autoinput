# -*- coding: utf-8 -*-
"""
ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ
web-scraping.devì—ì„œ í•™ìŠµí•œ ê¸°ìˆ ë“¤ì„ ëª¨ë“ˆí™”
"""
import requests
from bs4 import BeautifulSoup
import json
import time
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
import re
from dataclasses import dataclass
from enum import Enum

class ScrapingMethod(Enum):
    """ìŠ¤í¬ë˜í•‘ ë°©ë²• ì—´ê±°í˜•"""
    STATIC = "static"
    DYNAMIC = "dynamic"
    API = "api"
    HYBRID = "hybrid"

@dataclass
class ScrapingTarget:
    """ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì •ë³´"""
    url: str
    method: ScrapingMethod
    requires_auth: bool = False
    has_pagination: bool = False
    has_anti_bot: bool = False

class AdvancedScraper:
    """ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "https://web-scraping.dev"):
        self.base_url = base_url
        self.session = requests.Session()
        self._setup_headers()
        self.rate_limit_delay = 0.5
        
    def _setup_headers(self):
        """ê¸°ë³¸ í—¤ë” ì„¤ì •"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def scrape_with_pagination(self, url_pattern: str, max_pages: int = None) -> List[Dict]:
        """í˜ì´ì§€ë„¤ì´ì…˜ì´ ìˆëŠ” í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        all_data = []
        page = 1
        
        while True:
            if max_pages and page > max_pages:
                break
            
            url = url_pattern.format(page=page)
            response = self.session.get(url)
            
            if response.status_code != 200:
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            data = self._extract_products(soup)
            
            if not data:
                break
            
            all_data.extend(data)
            print(f"  ğŸ“„ í˜ì´ì§€ {page}: {len(data)}ê°œ í•­ëª© ìˆ˜ì§‘")
            
            page += 1
            time.sleep(self.rate_limit_delay)
        
        return all_data
    
    def _extract_products(self, soup: BeautifulSoup) -> List[Dict]:
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        
        # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
        selectors = [
            ('div.product', 'h3', 'span.price'),
            ('article.product-card', 'h2', 'p.price'),
            ('li.product-item', 'a.title', 'span.cost')
        ]
        
        for container_sel, title_sel, price_sel in selectors:
            containers = soup.select(container_sel)
            if containers:
                for container in containers:
                    title = container.select_one(title_sel)
                    price = container.select_one(price_sel)
                    
                    if title:
                        product = {
                            'title': title.text.strip(),
                            'price': price.text.strip() if price else 'N/A'
                        }
                        
                        # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                        img = container.select_one('img')
                        if img:
                            product['image'] = urljoin(self.base_url, img.get('src', ''))
                        
                        link = container.select_one('a[href]')
                        if link:
                            product['url'] = urljoin(self.base_url, link['href'])
                        
                        products.append(product)
                break
        
        return products
    
    def scrape_api(self, endpoint: str, params: Dict = None) -> Any:
        """API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤í¬ë˜í•‘"""
        url = urljoin(self.base_url, endpoint)
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            # JSON ì‘ë‹µ ì²˜ë¦¬
            if 'application/json' in response.headers.get('Content-Type', ''):
                data = response.json()
                
                # ë‹¤ì–‘í•œ API ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
                if isinstance(data, dict):
                    # products í‚¤ê°€ ìˆëŠ” ê²½ìš°
                    if 'products' in data:
                        return data['products']
                    # data í‚¤ê°€ ìˆëŠ” ê²½ìš°
                    elif 'data' in data:
                        return data['data']
                    # results í‚¤ê°€ ìˆëŠ” ê²½ìš°
                    elif 'results' in data:
                        return data['results']
                    else:
                        return data
                elif isinstance(data, list):
                    return data
                else:
                    return data
            else:
                return response.text
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_hidden_data(self, url: str) -> Dict:
        """ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ"""
        response = self.session.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        hidden_data = {}
        
        # 1. JavaScript ë³€ìˆ˜ì—ì„œ ì¶”ì¶œ
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                # window.* íŒ¨í„´ ì°¾ê¸°
                window_vars = re.findall(r'window\.(\w+)\s*=\s*({[^}]+}|\[[^\]]+\])', script.string)
                for var_name, var_value in window_vars:
                    try:
                        hidden_data[f'js_{var_name}'] = json.loads(var_value)
                    except:
                        hidden_data[f'js_{var_name}'] = var_value
                
                # var/let/const ë³€ìˆ˜ ì°¾ê¸°
                js_vars = re.findall(r'(?:var|let|const)\s+(\w+)\s*=\s*({[^}]+}|\[[^\]]+\])', script.string)
                for var_name, var_value in js_vars:
                    try:
                        hidden_data[f'var_{var_name}'] = json.loads(var_value)
                    except:
                        hidden_data[f'var_{var_name}'] = var_value
        
        # 2. data-* ì†ì„±ì—ì„œ ì¶”ì¶œ
        elements_with_data = soup.find_all(attrs=lambda x: x and x.startswith('data-'))
        for elem in elements_with_data:
            for attr, value in elem.attrs.items():
                if attr.startswith('data-'):
                    hidden_data[attr] = value
        
        # 3. JSON-LD êµ¬ì¡°í™” ë°ì´í„°
        json_ld = soup.find_all('script', type='application/ld+json')
        for idx, script in enumerate(json_ld):
            try:
                hidden_data[f'json_ld_{idx}'] = json.loads(script.string)
            except:
                pass
        
        # 4. ë©”íƒ€ íƒœê·¸
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('property'):
                hidden_data[f"meta_{meta['property']}"] = meta.get('content', '')
            elif meta.get('name'):
                hidden_data[f"meta_{meta['name']}"] = meta.get('content', '')
        
        return hidden_data
    
    def handle_infinite_scroll(self, url: str, max_items: int = 50) -> List[Dict]:
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬ (API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜)"""
        items = []
        page = 1
        per_page = 10
        
        while len(items) < max_items:
            # ì‹¤ì œë¡œëŠ” JavaScript ì‹¤í–‰ì´ í•„ìš”í•˜ì§€ë§Œ, API í˜¸ì¶œë¡œ ì‹œë®¬ë ˆì´ì…˜
            api_url = f"{url}/api/items"
            params = {
                'page': page,
                'per_page': per_page
            }
            
            data = self.scrape_api(api_url, params)
            
            if not data:
                break
            
            items.extend(data)
            page += 1
            time.sleep(self.rate_limit_delay)
        
        return items[:max_items]
    
    def bypass_cloudflare(self, url: str) -> Optional[str]:
        """Cloudflare ìš°íšŒ ì‹œë„ (ê¸°ë³¸ì ì¸ ë°©ë²•)"""
        # ì‹¤ì œ Cloudflare ìš°íšŒëŠ” ë” ë³µì¡í•œ ê¸°ìˆ ì´ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ë°©ë²•ë§Œ êµ¬í˜„
        
        headers = self.session.headers.copy()
        headers.update({
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        })
        
        try:
            response = self.session.get(url, headers=headers)
            
            # Cloudflare ì²´í¬
            if 'cf-ray' in response.headers:
                print("âš ï¸ Cloudflare ê°ì§€ë¨")
                # ì—¬ê¸°ì— ì¶”ê°€ ìš°íšŒ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥
            
            return response.text
            
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def scrape_with_cookies(self, url: str, cookies: Dict) -> Optional[str]:
        """ì¿ í‚¤ë¥¼ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        self.session.cookies.update(cookies)
        
        try:
            response = self.session.get(url)
            return response.text
        except Exception as e:
            print(f"âŒ ì¿ í‚¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def scrape_with_referer(self, url: str, referer: str) -> Optional[str]:
        """Referer í—¤ë”ë¥¼ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        headers = {'Referer': referer}
        
        try:
            response = self.session.get(url, headers=headers)
            return response.text
        except Exception as e:
            print(f"âŒ Referer ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None

class ScrapingOrchestrator:
    """ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¡°ìœ¨ì"""
    
    def __init__(self):
        self.scraper = AdvancedScraper()
        self.targets = []
    
    def add_target(self, target: ScrapingTarget):
        """ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì¶”ê°€"""
        self.targets.append(target)
    
    def execute(self) -> Dict[str, Any]:
        """ëª¨ë“  ëŒ€ìƒ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        results = {}
        
        for target in self.targets:
            print(f"\nğŸ¯ ìŠ¤í¬ë˜í•‘: {target.url}")
            
            if target.method == ScrapingMethod.API:
                data = self.scraper.scrape_api(target.url)
            elif target.has_pagination:
                data = self.scraper.scrape_with_pagination(target.url)
            else:
                response = self.scraper.session.get(target.url)
                soup = BeautifulSoup(response.text, 'html.parser')
                data = self.scraper._extract_products(soup)
            
            results[target.url] = data
            time.sleep(0.5)
        
        return results

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ê³ ê¸‰ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = AdvancedScraper()
    
    print("ğŸš€ ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # 1. API ìŠ¤í¬ë˜í•‘
    print("\nğŸ“¡ API ìŠ¤í¬ë˜í•‘:")
    api_data = scraper.scrape_api('/api/products')
    if api_data:
        print(f"  âœ… {len(api_data) if isinstance(api_data, list) else 1}ê°œ ë°ì´í„° ìˆ˜ì§‘")
    
    # 2. í˜ì´ì§€ë„¤ì´ì…˜
    print("\nğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ ìŠ¤í¬ë˜í•‘:")
    products = scraper.scrape_with_pagination(
        "https://web-scraping.dev/products?page={page}",
        max_pages=2
    )
    print(f"  âœ… ì´ {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘")
    
    # 3. ìˆ¨ê²¨ì§„ ë°ì´í„°
    print("\nğŸ” ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ:")
    hidden = scraper.extract_hidden_data("https://web-scraping.dev/products/1")
    print(f"  âœ… {len(hidden)}ê°œ ìˆ¨ê²¨ì§„ ë°ì´í„° ë°œê²¬")
    
    print("\n" + "=" * 80)
    print("âœ¨ ê³ ê¸‰ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ ì¤€ë¹„ ì™„ë£Œ!")