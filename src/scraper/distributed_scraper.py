# -*- coding: utf-8 -*-
"""
ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì•„í‚¤í…ì²˜
ëŒ€ê·œëª¨ ìŠ¤í¬ë˜í•‘ì„ ìœ„í•œ ë¶„ì‚° ì²˜ë¦¬ ì‹œìŠ¤í…œ
"""
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import json
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from queue import Queue, Empty
import threading
import pickle
import redis
from datetime import datetime, timedelta

class TaskStatus(Enum):
    """ì‘ì—… ìƒíƒœ"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"

@dataclass
class ScrapingTask:
    """ìŠ¤í¬ë˜í•‘ ì‘ì—… ë‹¨ìœ„"""
    task_id: str
    url: str
    method: str = "GET"
    headers: Dict = None
    data: Dict = None
    retry_count: int = 0
    max_retries: int = 3
    status: TaskStatus = TaskStatus.PENDING
    result: Any = None
    error: str = None
    created_at: datetime = None
    completed_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

class TaskQueue:
    """ì‘ì—… í ê´€ë¦¬"""
    
    def __init__(self, use_redis: bool = False, redis_host: str = 'localhost'):
        self.use_redis = use_redis
        
        if use_redis:
            try:
                self.redis_client = redis.Redis(host=redis_host, port=6379, db=0)
                self.redis_client.ping()
                print("âœ… Redis ì—°ê²° ì„±ê³µ")
            except:
                print("âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨, ë¡œì»¬ í ì‚¬ìš©")
                self.use_redis = False
                self.local_queue = Queue()
        else:
            self.local_queue = Queue()
    
    def push(self, task: ScrapingTask):
        """ì‘ì—… ì¶”ê°€"""
        if self.use_redis:
            serialized = pickle.dumps(task)
            self.redis_client.rpush('scraping_tasks', serialized)
        else:
            self.local_queue.put(task)
    
    def pop(self, timeout: int = 1) -> Optional[ScrapingTask]:
        """ì‘ì—… ê°€ì ¸ì˜¤ê¸°"""
        if self.use_redis:
            result = self.redis_client.blpop('scraping_tasks', timeout=timeout)
            if result:
                return pickle.loads(result[1])
        else:
            try:
                return self.local_queue.get(timeout=timeout)
            except Empty:
                return None
    
    def size(self) -> int:
        """í í¬ê¸°"""
        if self.use_redis:
            return self.redis_client.llen('scraping_tasks')
        else:
            return self.local_queue.qsize()

class AsyncScraper:
    """ë¹„ë™ê¸° ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, max_concurrent: int = 10):
        self.max_concurrent = max_concurrent
        self.semaphore = None
        self.session = None
    
    async def __aenter__(self):
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
    
    async def fetch(self, task: ScrapingTask) -> ScrapingTask:
        """ë¹„ë™ê¸° í˜ì¹˜"""
        async with self.semaphore:
            try:
                task.status = TaskStatus.RUNNING
                
                async with self.session.request(
                    task.method,
                    task.url,
                    headers=task.headers,
                    data=task.data,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        content_type = response.headers.get('Content-Type', '')
                        
                        if 'application/json' in content_type:
                            task.result = await response.json()
                        else:
                            task.result = await response.text()
                        
                        task.status = TaskStatus.COMPLETED
                    else:
                        task.error = f"HTTP {response.status}"
                        task.status = TaskStatus.FAILED
                
            except asyncio.TimeoutError:
                task.error = "Timeout"
                task.status = TaskStatus.FAILED
            except Exception as e:
                task.error = str(e)
                task.status = TaskStatus.FAILED
            
            finally:
                task.completed_at = datetime.now()
            
            return task
    
    async def batch_fetch(self, tasks: List[ScrapingTask]) -> List[ScrapingTask]:
        """ë°°ì¹˜ í˜ì¹˜"""
        results = await asyncio.gather(
            *[self.fetch(task) for task in tasks],
            return_exceptions=True
        )
        
        return [r if isinstance(r, ScrapingTask) else tasks[i] 
                for i, r in enumerate(results)]

class Worker(threading.Thread):
    """ì›Œì»¤ ìŠ¤ë ˆë“œ"""
    
    def __init__(self, worker_id: int, task_queue: TaskQueue, result_queue: Queue,
                 scraper_func: Callable):
        super().__init__()
        self.worker_id = worker_id
        self.task_queue = task_queue
        self.result_queue = result_queue
        self.scraper_func = scraper_func
        self.running = True
        self.processed_count = 0
    
    def run(self):
        """ì›Œì»¤ ì‹¤í–‰"""
        print(f"ğŸ”§ Worker {self.worker_id} ì‹œì‘")
        
        while self.running:
            task = self.task_queue.pop(timeout=1)
            
            if task is None:
                continue
            
            # ì‘ì—… ì²˜ë¦¬
            try:
                result = self.scraper_func(task)
                self.result_queue.put(result)
                self.processed_count += 1
                
                if self.processed_count % 10 == 0:
                    print(f"  Worker {self.worker_id}: {self.processed_count}ê°œ ì²˜ë¦¬")
                
            except Exception as e:
                task.error = str(e)
                task.status = TaskStatus.FAILED
                self.result_queue.put(task)
    
    def stop(self):
        """ì›Œì»¤ ì •ì§€"""
        self.running = False

class DistributedScraper:
    """ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì¡°ìœ¨ì"""
    
    def __init__(self, num_workers: int = 4, use_async: bool = True,
                 use_multiprocessing: bool = False):
        self.num_workers = num_workers
        self.use_async = use_async
        self.use_multiprocessing = use_multiprocessing
        
        self.task_queue = TaskQueue()
        self.result_queue = Queue()
        self.workers = []
        
        self.stats = {
            'total': 0,
            'completed': 0,
            'failed': 0,
            'retried': 0,
            'start_time': None,
            'end_time': None
        }
    
    def add_task(self, url: str, **kwargs) -> str:
        """ì‘ì—… ì¶”ê°€"""
        task_id = f"task_{self.stats['total']}_{int(time.time())}"
        task = ScrapingTask(
            task_id=task_id,
            url=url,
            **kwargs
        )
        
        self.task_queue.push(task)
        self.stats['total'] += 1
        
        return task_id
    
    def add_urls(self, urls: List[str]):
        """URL ë¦¬ìŠ¤íŠ¸ ì¶”ê°€"""
        task_ids = []
        for url in urls:
            task_id = self.add_task(url)
            task_ids.append(task_id)
        
        return task_ids
    
    async def run_async(self) -> List[ScrapingTask]:
        """ë¹„ë™ê¸° ì‹¤í–‰"""
        print(f"\nğŸš€ ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ë™ì‹œ ì‹¤í–‰: {self.num_workers})")
        self.stats['start_time'] = datetime.now()
        
        results = []
        tasks = []
        
        # ëª¨ë“  ì‘ì—… ìˆ˜ì§‘
        while True:
            task = self.task_queue.pop(timeout=0)
            if task is None:
                break
            tasks.append(task)
        
        # ë°°ì¹˜ ì²˜ë¦¬
        async with AsyncScraper(max_concurrent=self.num_workers) as scraper:
            batch_size = self.num_workers * 2
            
            for i in range(0, len(tasks), batch_size):
                batch = tasks[i:i+batch_size]
                batch_results = await scraper.batch_fetch(batch)
                
                for result in batch_results:
                    if result.status == TaskStatus.COMPLETED:
                        self.stats['completed'] += 1
                    elif result.status == TaskStatus.FAILED:
                        if result.retry_count < result.max_retries:
                            result.retry_count += 1
                            result.status = TaskStatus.RETRYING
                            self.stats['retried'] += 1
                            self.task_queue.push(result)
                        else:
                            self.stats['failed'] += 1
                
                results.extend(batch_results)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                print(f"  ì§„í–‰: {len(results)}/{len(tasks)} "
                      f"(ì„±ê³µ: {self.stats['completed']}, ì‹¤íŒ¨: {self.stats['failed']})")
        
        self.stats['end_time'] = datetime.now()
        return results
    
    def run_threaded(self, scraper_func: Callable) -> List[ScrapingTask]:
        """ìŠ¤ë ˆë“œ ê¸°ë°˜ ì‹¤í–‰"""
        print(f"\nğŸ”„ ìŠ¤ë ˆë“œ ê¸°ë°˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ì›Œì»¤: {self.num_workers})")
        self.stats['start_time'] = datetime.now()
        
        # ì›Œì»¤ ìƒì„±
        for i in range(self.num_workers):
            worker = Worker(i, self.task_queue, self.result_queue, scraper_func)
            worker.start()
            self.workers.append(worker)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        while len(results) < self.stats['total']:
            try:
                result = self.result_queue.get(timeout=1)
                results.append(result)
                
                if result.status == TaskStatus.COMPLETED:
                    self.stats['completed'] += 1
                elif result.status == TaskStatus.FAILED:
                    self.stats['failed'] += 1
                
                # ì§„í–‰ ìƒí™©
                if len(results) % 10 == 0:
                    print(f"  ìˆ˜ì§‘: {len(results)}/{self.stats['total']}")
                
            except Empty:
                # ëª¨ë“  ì›Œì»¤ê°€ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                if all(not w.is_alive() for w in self.workers):
                    break
        
        # ì›Œì»¤ ì •ì§€
        for worker in self.workers:
            worker.stop()
        
        for worker in self.workers:
            worker.join()
        
        self.stats['end_time'] = datetime.now()
        return results
    
    def run_multiprocess(self, scraper_func: Callable) -> List[ScrapingTask]:
        """ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print(f"\nâš¡ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (í”„ë¡œì„¸ìŠ¤: {self.num_workers})")
        self.stats['start_time'] = datetime.now()
        
        # ì‘ì—… ìˆ˜ì§‘
        tasks = []
        while True:
            task = self.task_queue.pop(timeout=0)
            if task is None:
                break
            tasks.append(task)
        
        results = []
        
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            # ì‘ì—… ì œì¶œ
            futures = {executor.submit(scraper_func, task): task 
                      for task in tasks}
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result.status == TaskStatus.COMPLETED:
                        self.stats['completed'] += 1
                    else:
                        self.stats['failed'] += 1
                    
                    # ì§„í–‰ ìƒí™©
                    if len(results) % 10 == 0:
                        print(f"  ì²˜ë¦¬: {len(results)}/{len(tasks)}")
                        
                except Exception as e:
                    task = futures[future]
                    task.error = str(e)
                    task.status = TaskStatus.FAILED
                    results.append(task)
                    self.stats['failed'] += 1
        
        self.stats['end_time'] = datetime.now()
        return results
    
    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        
        if stats['start_time'] and stats['end_time']:
            duration = (stats['end_time'] - stats['start_time']).total_seconds()
            stats['duration'] = f"{duration:.2f}ì´ˆ"
            stats['speed'] = f"{stats['completed'] / duration:.2f} req/s" if duration > 0 else "N/A"
        
        return stats
    
    def print_summary(self):
        """ìš”ì•½ ì¶œë ¥"""
        stats = self.get_stats()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ìŠ¤í¬ë˜í•‘ ìš”ì•½")
        print("-" * 40)
        print(f"  ì´ ì‘ì—…: {stats['total']}")
        print(f"  ì™„ë£Œ: {stats['completed']} ({stats['completed']/stats['total']*100:.1f}%)")
        print(f"  ì‹¤íŒ¨: {stats['failed']}")
        print(f"  ì¬ì‹œë„: {stats['retried']}")
        
        if 'duration' in stats:
            print(f"  ì†Œìš” ì‹œê°„: {stats['duration']}")
            print(f"  ì²˜ë¦¬ ì†ë„: {stats['speed']}")
        
        print("=" * 80)

# ìƒ˜í”Œ ìŠ¤í¬ë˜í¼ í•¨ìˆ˜
def simple_scraper(task: ScrapingTask) -> ScrapingTask:
    """ê°„ë‹¨í•œ ë™ê¸° ìŠ¤í¬ë˜í¼"""
    import requests
    
    try:
        response = requests.get(task.url, headers=task.headers, timeout=10)
        
        if response.status_code == 200:
            task.result = response.text[:1000]  # ì²˜ìŒ 1000ìë§Œ
            task.status = TaskStatus.COMPLETED
        else:
            task.error = f"HTTP {response.status_code}"
            task.status = TaskStatus.FAILED
            
    except Exception as e:
        task.error = str(e)
        task.status = TaskStatus.FAILED
    
    task.completed_at = datetime.now()
    return task

# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ URLë“¤
    test_urls = [
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/2",
        "https://httpbin.org/json",
        "https://httpbin.org/html",
        "https://httpbin.org/status/200"
    ] * 2  # 10ê°œ ì‘ì—…
    
    # ë¶„ì‚° ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = DistributedScraper(num_workers=3)
    
    # ì‘ì—… ì¶”ê°€
    print(f"\nğŸ“ {len(test_urls)}ê°œ ì‘ì—… ì¶”ê°€")
    scraper.add_urls(test_urls)
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    if scraper.use_async:
        results = await scraper.run_async()
    else:
        results = scraper.run_threaded(simple_scraper)
    
    # ê²°ê³¼ ìš”ì•½
    scraper.print_summary()
    
    print("\nâœ¨ ë¶„ì‚° ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")

if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())