# -*- coding: utf-8 -*-
"""
web-scraping.dev ì™„ë²½ íƒìƒ‰ ë° í•™ìŠµ ëª¨ë“ˆ
ëª¨ë“  ì±Œë¦°ì§€ì™€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµ
"""
import requests
from bs4 import BeautifulSoup
import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import re
from urllib.parse import urljoin, urlparse

class ChallengeType(Enum):
    """ì±Œë¦°ì§€ ìœ í˜•"""
    PAGING = "paging"
    AUTH = "authentication"
    DATA_EXTRACTION = "data_extraction"
    ANTI_SCRAPING = "anti_scraping"
    DYNAMIC = "dynamic_content"
    API = "api"
    ADVANCED = "advanced"

@dataclass
class Challenge:
    """ì±Œë¦°ì§€ ì •ë³´"""
    name: str
    url: str
    type: ChallengeType
    difficulty: str
    description: str
    solution: Dict = None
    learned: bool = False

class WebScrapingDevMaster:
    """web-scraping.dev ì™„ë²½ ë§ˆìŠ¤í„° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = "https://web-scraping.dev"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.challenges = []
        self.solutions = {}
        
    def discover_all_challenges(self) -> List[Challenge]:
        """ëª¨ë“  ì±Œë¦°ì§€ ë°œê²¬ ë° ë¶„ë¥˜"""
        print("\nğŸ” web-scraping.dev ì „ì²´ ì±Œë¦°ì§€ íƒìƒ‰ ì¤‘...")
        
        response = self.session.get(self.base_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ëª¨ë“  ì±Œë¦°ì§€ ì„¹ì…˜ ì°¾ê¸°
        challenge_sections = {
            'paging': self._find_paging_challenges(soup),
            'auth': self._find_auth_challenges(soup),
            'data': self._find_data_challenges(soup),
            'anti': self._find_anti_scraping_challenges(soup),
            'dynamic': self._find_dynamic_challenges(soup)
        }
        
        # ì±Œë¦°ì§€ ê°ì²´ ìƒì„±
        for category, items in challenge_sections.items():
            for item in items:
                challenge = Challenge(
                    name=item['name'],
                    url=item['url'],
                    type=self._get_challenge_type(category),
                    difficulty=item.get('difficulty', 'medium'),
                    description=item.get('description', '')
                )
                self.challenges.append(challenge)
        
        print(f"âœ… ì´ {len(self.challenges)}ê°œ ì±Œë¦°ì§€ ë°œê²¬!")
        return self.challenges
    
    def _find_paging_challenges(self, soup) -> List[Dict]:
        """í˜ì´ì§• ì±Œë¦°ì§€ ì°¾ê¸°"""
        challenges = []
        
        # í˜ì´ì§• ê´€ë ¨ ë§í¬ íŒ¨í„´
        paging_patterns = [
            r'.*page.*', r'.*paging.*', r'.*pagination.*', 
            r'.*endless.*', r'.*infinite.*', r'.*scroll.*',
            r'.*load.*more.*'
        ]
        
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            for pattern in paging_patterns:
                if re.match(pattern, href.lower()) or re.match(pattern, text.lower()):
                    challenges.append({
                        'name': text or href,
                        'url': urljoin(self.base_url, href),
                        'difficulty': self._assess_difficulty(text)
                    })
                    break
        
        return challenges
    
    def _find_auth_challenges(self, soup) -> List[Dict]:
        """ì¸ì¦ ì±Œë¦°ì§€ ì°¾ê¸°"""
        challenges = []
        
        auth_patterns = [
            r'.*login.*', r'.*auth.*', r'.*csrf.*',
            r'.*token.*', r'.*session.*', r'.*cookie.*'
        ]
        
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            for pattern in auth_patterns:
                if re.match(pattern, href.lower()) or re.match(pattern, text.lower()):
                    challenges.append({
                        'name': text or href,
                        'url': urljoin(self.base_url, href),
                        'difficulty': 'hard' if 'csrf' in text.lower() else 'medium'
                    })
                    break
        
        return challenges
    
    def _find_data_challenges(self, soup) -> List[Dict]:
        """ë°ì´í„° ì¶”ì¶œ ì±Œë¦°ì§€ ì°¾ê¸°"""
        challenges = []
        
        data_patterns = [
            r'.*json.*', r'.*hidden.*', r'.*extract.*',
            r'.*pdf.*', r'.*download.*', r'.*storage.*'
        ]
        
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            for pattern in data_patterns:
                if re.match(pattern, href.lower()) or re.match(pattern, text.lower()):
                    challenges.append({
                        'name': text or href,
                        'url': urljoin(self.base_url, href),
                        'difficulty': 'medium'
                    })
                    break
        
        return challenges
    
    def _find_anti_scraping_challenges(self, soup) -> List[Dict]:
        """Anti-scraping ì±Œë¦°ì§€ ì°¾ê¸°"""
        challenges = []
        
        anti_patterns = [
            r'.*block.*', r'.*protect.*', r'.*captcha.*',
            r'.*redirect.*', r'.*referer.*', r'.*rate.*limit.*'
        ]
        
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            for pattern in anti_patterns:
                if re.match(pattern, href.lower()) or re.match(pattern, text.lower()):
                    challenges.append({
                        'name': text or href,
                        'url': urljoin(self.base_url, href),
                        'difficulty': 'hard'
                    })
                    break
        
        return challenges
    
    def _find_dynamic_challenges(self, soup) -> List[Dict]:
        """ë™ì  ì½˜í…ì¸  ì±Œë¦°ì§€ ì°¾ê¸°"""
        challenges = []
        
        dynamic_patterns = [
            r'.*javascript.*', r'.*ajax.*', r'.*spa.*',
            r'.*react.*', r'.*vue.*', r'.*angular.*',
            r'.*graphql.*', r'.*websocket.*'
        ]
        
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            text = link.get_text(strip=True)
            
            for pattern in dynamic_patterns:
                if re.match(pattern, href.lower()) or re.match(pattern, text.lower()):
                    challenges.append({
                        'name': text or href,
                        'url': urljoin(self.base_url, href),
                        'difficulty': 'hard'
                    })
                    break
        
        return challenges
    
    def _get_challenge_type(self, category: str) -> ChallengeType:
        """ì¹´í…Œê³ ë¦¬ë¥¼ ì±Œë¦°ì§€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        mapping = {
            'paging': ChallengeType.PAGING,
            'auth': ChallengeType.AUTH,
            'data': ChallengeType.DATA_EXTRACTION,
            'anti': ChallengeType.ANTI_SCRAPING,
            'dynamic': ChallengeType.DYNAMIC
        }
        return mapping.get(category, ChallengeType.ADVANCED)
    
    def _assess_difficulty(self, text: str) -> str:
        """ë‚œì´ë„ í‰ê°€"""
        hard_keywords = ['advanced', 'complex', 'csrf', 'captcha', 'graphql']
        easy_keywords = ['simple', 'basic', 'intro', 'static']
        
        text_lower = text.lower()
        
        for keyword in hard_keywords:
            if keyword in text_lower:
                return 'hard'
        
        for keyword in easy_keywords:
            if keyword in text_lower:
                return 'easy'
        
        return 'medium'
    
    def solve_challenge(self, challenge: Challenge) -> Dict:
        """ì±Œë¦°ì§€ í•´ê²°"""
        print(f"\nğŸ¯ ì±Œë¦°ì§€ í•´ê²°: {challenge.name}")
        print(f"   URL: {challenge.url}")
        print(f"   ìœ í˜•: {challenge.type.value}")
        print(f"   ë‚œì´ë„: {challenge.difficulty}")
        
        solution = {}
        
        try:
            if challenge.type == ChallengeType.PAGING:
                solution = self._solve_paging_challenge(challenge)
            elif challenge.type == ChallengeType.AUTH:
                solution = self._solve_auth_challenge(challenge)
            elif challenge.type == ChallengeType.DATA_EXTRACTION:
                solution = self._solve_data_challenge(challenge)
            elif challenge.type == ChallengeType.ANTI_SCRAPING:
                solution = self._solve_anti_scraping_challenge(challenge)
            elif challenge.type == ChallengeType.DYNAMIC:
                solution = self._solve_dynamic_challenge(challenge)
            else:
                solution = self._solve_advanced_challenge(challenge)
            
            challenge.solution = solution
            challenge.learned = True
            self.solutions[challenge.name] = solution
            
            print(f"   âœ… í•´ê²° ì™„ë£Œ!")
            
        except Exception as e:
            print(f"   âŒ í•´ê²° ì‹¤íŒ¨: {e}")
            solution = {'error': str(e)}
        
        return solution
    
    def _solve_paging_challenge(self, challenge: Challenge) -> Dict:
        """í˜ì´ì§• ì±Œë¦°ì§€ í•´ê²°"""
        solution = {
            'type': 'paging',
            'method': '',
            'code': '',
            'data': []
        }
        
        # í˜ì´ì§€ ë¶„ì„
        response = self.session.get(challenge.url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # í˜ì´ì§• ìœ í˜• ê°ì§€
        if soup.find('button', text=re.compile('load more', re.I)):
            solution['method'] = 'load_more_button'
            solution['code'] = """
# Load More ë²„íŠ¼ ì²˜ë¦¬
while True:
    load_more = soup.find('button', text='Load More')
    if not load_more:
        break
    # AJAX ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
    next_url = load_more.get('data-url')
    response = session.post(next_url)
    new_items = response.json()['items']
    data.extend(new_items)
"""
        elif soup.find('div', {'class': re.compile('infinite|endless')}):
            solution['method'] = 'infinite_scroll'
            solution['code'] = """
# ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
offset = 0
while True:
    api_url = f'/api/items?offset={offset}&limit=20'
    response = session.get(api_url)
    items = response.json()['items']
    if not items:
        break
    data.extend(items)
    offset += 20
"""
        else:
            solution['method'] = 'traditional_pagination'
            solution['code'] = """
# ì „í†µì  í˜ì´ì§€ë„¤ì´ì…˜
page = 1
while True:
    url = f'{base_url}?page={page}'
    response = session.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    items = soup.find_all('div', class_='item')
    if not items:
        break
    data.extend(items)
    page += 1
"""
        
        return solution
    
    def _solve_auth_challenge(self, challenge: Challenge) -> Dict:
        """ì¸ì¦ ì±Œë¦°ì§€ í•´ê²°"""
        solution = {
            'type': 'authentication',
            'method': '',
            'code': '',
            'headers': {}
        }
        
        response = self.session.get(challenge.url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì¸ì¦ ìœ í˜• ê°ì§€
        if 'csrf' in challenge.url.lower():
            csrf_token = soup.find('input', {'name': 'csrf_token'})
            if csrf_token:
                solution['method'] = 'csrf_token'
                solution['code'] = """
# CSRF í† í° ì²˜ë¦¬
csrf_token = soup.find('input', {'name': 'csrf_token'})['value']
login_data = {
    'username': 'user',
    'password': 'pass',
    'csrf_token': csrf_token
}
response = session.post(login_url, data=login_data)
"""
        elif 'cookie' in challenge.url.lower():
            solution['method'] = 'cookie_auth'
            solution['code'] = """
# ì¿ í‚¤ ì¸ì¦
session.cookies.set('auth_token', 'secret_token')
response = session.get(protected_url)
"""
        elif 'api' in challenge.url.lower():
            solution['method'] = 'api_key'
            solution['headers'] = {'X-API-Key': 'your_api_key'}
            solution['code'] = """
# API í‚¤ ì¸ì¦
headers = {'X-API-Key': 'your_api_key'}
response = session.get(api_url, headers=headers)
"""
        
        return solution
    
    def _solve_data_challenge(self, challenge: Challenge) -> Dict:
        """ë°ì´í„° ì¶”ì¶œ ì±Œë¦°ì§€ í•´ê²°"""
        solution = {
            'type': 'data_extraction',
            'method': '',
            'code': '',
            'extracted_data': {}
        }
        
        response = self.session.get(challenge.url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìˆ¨ê²¨ì§„ ë°ì´í„° ì°¾ê¸°
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'window.' in script.string:
                # JavaScript ë³€ìˆ˜ ì¶”ì¶œ
                matches = re.findall(r'window\.(\w+)\s*=\s*({.*?});', 
                                   script.string, re.DOTALL)
                for var_name, json_str in matches:
                    try:
                        data = json.loads(json_str)
                        solution['extracted_data'][var_name] = data
                        solution['method'] = 'javascript_extraction'
                    except:
                        pass
        
        # JSON-LD ë°ì´í„°
        json_ld = soup.find_all('script', type='application/ld+json')
        if json_ld:
            solution['method'] = 'json_ld_extraction'
            for script in json_ld:
                try:
                    data = json.loads(script.string)
                    solution['extracted_data']['json_ld'] = data
                except:
                    pass
        
        solution['code'] = """
# ìˆ¨ê²¨ì§„ ë°ì´í„° ì¶”ì¶œ
# 1. JavaScript ë³€ìˆ˜
scripts = soup.find_all('script')
for script in scripts:
    if script.string:
        # window.* íŒ¨í„´ ì°¾ê¸°
        
# 2. data-* ì†ì„±
elements = soup.find_all(attrs={'data-product': True})
for elem in elements:
    product_data = json.loads(elem['data-product'])
    
# 3. JSON-LD
json_ld = soup.find('script', type='application/ld+json')
if json_ld:
    structured_data = json.loads(json_ld.string)
"""
        
        return solution
    
    def _solve_anti_scraping_challenge(self, challenge: Challenge) -> Dict:
        """Anti-scraping ì±Œë¦°ì§€ í•´ê²°"""
        solution = {
            'type': 'anti_scraping',
            'method': '',
            'bypass_technique': '',
            'code': ''
        }
        
        # ë‹¤ì–‘í•œ ìš°íšŒ ê¸°ë²• ì‹œë„
        if 'block' in challenge.url.lower():
            solution['method'] = 'blocking_bypass'
            solution['bypass_technique'] = 'User-Agent rotation + Headers'
            solution['code'] = """
# ì°¨ë‹¨ ìš°íšŒ
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}
response = session.get(url, headers=headers)
"""
        elif 'redirect' in challenge.url.lower():
            solution['method'] = 'redirect_handling'
            solution['bypass_technique'] = 'Follow redirects with session'
            solution['code'] = """
# ë¦¬ë””ë ‰ì…˜ ì²˜ë¦¬
session.max_redirects = 10
response = session.get(url, allow_redirects=True)
final_url = response.url
"""
        elif 'referer' in challenge.url.lower():
            solution['method'] = 'referer_spoofing'
            solution['bypass_technique'] = 'Set Referer header'
            solution['code'] = """
# Referer í—¤ë” ì„¤ì •
headers = {'Referer': 'https://google.com'}
response = session.get(url, headers=headers)
"""
        
        return solution
    
    def _solve_dynamic_challenge(self, challenge: Challenge) -> Dict:
        """ë™ì  ì½˜í…ì¸  ì±Œë¦°ì§€ í•´ê²°"""
        solution = {
            'type': 'dynamic_content',
            'method': '',
            'tools_needed': [],
            'code': ''
        }
        
        if 'graphql' in challenge.url.lower():
            solution['method'] = 'graphql_query'
            solution['tools_needed'] = ['requests', 'json']
            solution['code'] = """
# GraphQL ì¿¼ë¦¬
query = '''
query GetProducts {
    products {
        id
        name
        price
    }
}
'''
response = session.post(
    graphql_endpoint,
    json={'query': query}
)
data = response.json()['data']['products']
"""
        elif 'spa' in challenge.url.lower() or 'react' in challenge.url.lower():
            solution['method'] = 'spa_scraping'
            solution['tools_needed'] = ['selenium', 'playwright']
            solution['code'] = """
# SPA ìŠ¤í¬ë˜í•‘ (Selenium)
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get(url)

# ë™ì  ì½˜í…ì¸  ëŒ€ê¸°
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, 'product'))
)

# ë°ì´í„° ì¶”ì¶œ
products = driver.find_elements(By.CLASS_NAME, 'product')
"""
        else:
            solution['method'] = 'ajax_interception'
            solution['tools_needed'] = ['browser devtools', 'network analysis']
            solution['code'] = """
# AJAX ìš”ì²­ ì§ì ‘ í˜¸ì¶œ
# 1. DevToolsì—ì„œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
# 2. API ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°
# 3. ì§ì ‘ í˜¸ì¶œ

api_url = '/api/data'
response = session.get(api_url)
data = response.json()
"""
        
        return solution
    
    def _solve_advanced_challenge(self, challenge: Challenge) -> Dict:
        """ê³ ê¸‰ ì±Œë¦°ì§€ í•´ê²°"""
        return {
            'type': 'advanced',
            'method': 'custom_solution',
            'note': 'Requires specific analysis',
            'code': '# Custom solution needed'
        }
    
    def generate_complete_report(self) -> str:
        """ì™„ì „í•œ í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ“š web-scraping.dev ì™„ë²½ í•™ìŠµ ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        
        # ì±Œë¦°ì§€ ìš”ì•½
        report.append(f"\nğŸ“Š ì „ì²´ ì±Œë¦°ì§€: {len(self.challenges)}ê°œ")
        report.append(f"âœ… í•™ìŠµ ì™„ë£Œ: {sum(1 for c in self.challenges if c.learned)}ê°œ")
        report.append(f"âŒ ë¯¸í•™ìŠµ: {sum(1 for c in self.challenges if not c.learned)}ê°œ")
        
        # ìœ í˜•ë³„ ë¶„ë¥˜
        report.append("\nğŸ“‚ ìœ í˜•ë³„ ì±Œë¦°ì§€:")
        for challenge_type in ChallengeType:
            count = sum(1 for c in self.challenges if c.type == challenge_type)
            learned = sum(1 for c in self.challenges if c.type == challenge_type and c.learned)
            report.append(f"  â€¢ {challenge_type.value}: {learned}/{count}")
        
        # ì†”ë£¨ì…˜ ìƒì„¸
        report.append("\nğŸ”§ í•™ìŠµí•œ ì†”ë£¨ì…˜:")
        for challenge in self.challenges:
            if challenge.learned and challenge.solution:
                report.append(f"\n### {challenge.name}")
                report.append(f"- URL: {challenge.url}")
                report.append(f"- ìœ í˜•: {challenge.type.value}")
                report.append(f"- ë°©ë²•: {challenge.solution.get('method', 'N/A')}")
                if 'code' in challenge.solution:
                    report.append(f"- ì½”ë“œ ìŠ¤ë‹ˆí« í¬í•¨")
        
        return "\n".join(report)
    
    def export_solutions(self, filename: str = "webscraping_solutions.json"):
        """ì†”ë£¨ì…˜ ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_challenges': len(self.challenges),
            'learned_challenges': sum(1 for c in self.challenges if c.learned),
            'solutions': {}
        }
        
        for challenge in self.challenges:
            if challenge.learned:
                export_data['solutions'][challenge.name] = {
                    'url': challenge.url,
                    'type': challenge.type.value,
                    'difficulty': challenge.difficulty,
                    'solution': challenge.solution
                }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ì†”ë£¨ì…˜ ì €ì¥ ì™„ë£Œ: {filename}")

# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    print("ğŸš€ web-scraping.dev ì™„ë²½ ë§ˆìŠ¤í„° í”„ë¡œê·¸ë¨")
    print("=" * 80)
    
    # ë§ˆìŠ¤í„° í´ë˜ìŠ¤ ì´ˆê¸°í™”
    master = WebScrapingDevMaster()
    
    # 1. ëª¨ë“  ì±Œë¦°ì§€ ë°œê²¬
    challenges = master.discover_all_challenges()
    
    # 2. ê° ì±Œë¦°ì§€ í•´ê²° (ë°ëª¨: ì²˜ìŒ 5ê°œë§Œ)
    for challenge in challenges[:5]:
        master.solve_challenge(challenge)
        time.sleep(1)  # Rate limiting
    
    # 3. ë¦¬í¬íŠ¸ ìƒì„±
    report = master.generate_complete_report()
    print(report)
    
    # 4. ì†”ë£¨ì…˜ ì €ì¥
    master.export_solutions("webscraping_dev_solutions.json")
    
    print("\nâœ¨ í•™ìŠµ ì™„ë£Œ!")