#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë²•ë ¹, ì§ˆë¬¸, í†µê³„ ìë£Œì‹¤ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
ëª¨ë“  ìë£Œì‹¤ ê²Œì‹œíŒ í…ŒìŠ¤íŠ¸
"""

import asyncio
from playwright.async_api import async_playwright
from datetime import datetime
import os
import sys
import re
import json

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

class AllBoardsScraper:
    """ëª¨ë“  ìë£Œì‹¤ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.base_url = "https://longtermcare.or.kr"
        self.download_dir = "data/downloads/all_boards"
        self.results = {
            'ì„œì‹': {'posts': 0, 'files': 0},
            'ë²•ë ¹': {'posts': 0, 'files': 0},
            'ì§ˆë¬¸': {'posts': 0, 'files': 0},
            'í†µê³„': {'posts': 0, 'files': 0}
        }
        
        # ê° ê²Œì‹œíŒì˜ URLê³¼ communityKey
        self.boards = {
            'ì„œì‹': {
                'url': '/npbs/d/m/000/moveBoardView?menuId=npe0000000920&bKey=B0017',
                'communityKey': 'B0017',
                'name': 'ì„œì‹ìë£Œì‹¤'
            },
            'ë²•ë ¹': {
                'url': '/npbs/d/m/000/moveBoardView?menuId=npe0000000930&bKey=B0018',
                'communityKey': 'B0018',
                'name': 'ë²•ë ¹ìë£Œì‹¤'
            },
            'ì§ˆë¬¸': {
                'url': '/npbs/d/m/000/moveBoardView?menuId=npe0000000940&bKey=B0008',
                'communityKey': 'B0008',
                'name': 'ìì£¼í•˜ëŠ”ì§ˆë¬¸'
            },
            'í†µê³„': {
                'url': '/npbs/d/m/000/moveBoardView?menuId=npe0000000950&bKey=B0009',
                'communityKey': 'B0009',
                'name': 'í†µê³„ìë£Œì‹¤'
            }
        }
    
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        os.makedirs(self.download_dir, exist_ok=True)
        os.makedirs("logs/screenshots", exist_ok=True)
        
        # ê° ê²Œì‹œíŒë³„ ë‹¤ìš´ë¡œë“œ í´ë” ìƒì„±
        for board_name in self.boards.keys():
            board_dir = os.path.join(self.download_dir, board_name)
            os.makedirs(board_dir, exist_ok=True)
        
        print(f"[INIT] ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬: {self.download_dir}")
    
    async def get_board_posts(self, page, board_type):
        """ê²Œì‹œíŒ ëª©ë¡ì—ì„œ ê²Œì‹œë¬¼ ID ì¶”ì¶œ"""
        board_info = self.boards[board_type]
        
        print(f"\n{'='*60}")
        print(f"  {board_info['name']} ìŠ¤í¬ë˜í•‘")
        print(f"{'='*60}")
        
        # ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™
        list_url = self.base_url + board_info['url']
        print(f"\n[1] ëª©ë¡ í˜ì´ì§€ ì´ë™")
        print(f"    URL: {list_url[:80]}...")
        
        await page.goto(list_url, wait_until='networkidle')
        await page.wait_for_timeout(3000)
        
        # ìŠ¤í¬ë¦°ìƒ·
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        screenshot_path = f'logs/screenshots/{board_type}_list_{timestamp}.png'
        await page.screenshot(path=screenshot_path)
        print(f"    [ìŠ¤í¬ë¦°ìƒ·] {screenshot_path}")
        
        # ê²Œì‹œë¬¼ ID ì¶”ì¶œ - JavaScriptë¡œ ë§í¬ ë¶„ì„
        post_ids = await page.evaluate('''() => {
            const posts = [];
            const links = document.querySelectorAll('a[href*="boardId="]');
            
            for (let link of links) {
                const href = link.getAttribute('href');
                const match = href.match(/boardId=(\d+)/);
                if (match) {
                    const title = link.textContent.trim();
                    posts.push({
                        id: match[1],
                        title: title
                    });
                }
            }
            
            // ì¤‘ë³µ ì œê±°
            const unique = [];
            const seen = new Set();
            for (let post of posts) {
                if (!seen.has(post.id)) {
                    seen.add(post.id);
                    unique.push(post);
                }
            }
            
            return unique.slice(0, 3); // ìµœëŒ€ 3ê°œ
        }''')
        
        if not post_ids:
            # ë°±ì—… ë°©ë²•: onclick ì†ì„±ì—ì„œ ì¶”ì¶œ
            post_ids = await page.evaluate('''() => {
                const posts = [];
                const elements = document.querySelectorAll('[onclick*="fnViewDetail"], [onclick*="fnDetail"], [onclick*="view"]');
                
                for (let el of elements) {
                    const onclick = el.getAttribute('onclick');
                    const match = onclick.match(/['"](\d+)['"]/);
                    if (match) {
                        const title = el.textContent.trim();
                        posts.push({
                            id: match[1],
                            title: title
                        });
                    }
                }
                
                return posts.slice(0, 3);
            }''')
        
        print(f"    ì¶”ì¶œëœ ê²Œì‹œë¬¼: {len(post_ids)}ê°œ")
        for post in post_ids:
            print(f"      - ID: {post['id']}, ì œëª©: {post['title'][:30]}...")
        
        return post_ids, board_info['communityKey']
    
    async def download_post_attachments(self, page, board_type, post_id, community_key):
        """ê²Œì‹œë¬¼ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        
        # ìƒì„¸ í˜ì´ì§€ URL ìƒì„±
        detail_url = f"https://longtermcare.or.kr/npbs/cms/board/board/Board.jsp?searchType=ALL&searchWord=&list_start_date=&list_end_date=&pageSize=&branch_id=&branch_child_id=&pageNum=1&list_show_answer=N&communityKey={community_key}&boardId={post_id}&act=VIEW"
        
        print(f"\n  [ê²Œì‹œë¬¼ {post_id}]")
        print(f"    ìƒì„¸ í˜ì´ì§€ ì´ë™...")
        
        await page.goto(detail_url, wait_until='networkidle')
        await page.wait_for_timeout(2000)
        
        # ì²¨ë¶€íŒŒì¼ ì°¾ê¸°
        file_links = await page.query_selector_all('a[href*="/Download.jsp"], a[href*="file"], a[onclick*="download"]')
        
        if not file_links:
            # í™•ì¥ ê²€ìƒ‰
            all_links = await page.query_selector_all('a')
            file_links = []
            for link in all_links:
                try:
                    text = await link.text_content()
                    href = await link.get_attribute('href')
                    if text and any(ext in text.lower() for ext in ['.hwp', '.pdf', '.doc', '.xls', '.zip', '.ppt']):
                        file_links.append(link)
                    elif href and '/Download.jsp' in href:
                        file_links.append(link)
                except:
                    pass
        
        download_count = 0
        
        if file_links:
            print(f"    ì²¨ë¶€íŒŒì¼ {len(file_links)}ê°œ ë°œê²¬")
            
            # ë‹¤ìš´ë¡œë“œ í´ë” ìƒì„±
            post_folder = os.path.join(self.download_dir, board_type, f"post_{post_id}")
            os.makedirs(post_folder, exist_ok=True)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for idx, file_link in enumerate(file_links[:2], 1):  # ìµœëŒ€ 2ê°œ
                try:
                    file_text = await file_link.text_content()
                    file_name = file_text.strip() if file_text else f"file_{idx}"
                    
                    print(f"      íŒŒì¼ {idx}: {file_name[:40]}...")
                    
                    # ë‹¤ìš´ë¡œë“œ ì‹œë„
                    async with page.expect_download(timeout=30000) as download_info:
                        await file_link.click()
                    
                    download = await download_info.value
                    
                    # íŒŒì¼ ì €ì¥
                    suggested_name = download.suggested_filename
                    safe_name = re.sub(r'[<>:"/\\|?*]', '_', suggested_name)[:100]
                    file_path = os.path.join(post_folder, safe_name)
                    
                    await download.save_as(file_path)
                    
                    print(f"        âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {safe_name}")
                    download_count += 1
                    
                except Exception as e:
                    print(f"        âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)[:50]}")
        else:
            print(f"    ì²¨ë¶€íŒŒì¼ ì—†ìŒ")
        
        return download_count
    
    async def scrape_board(self, page, board_type):
        """ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘"""
        try:
            # 1. ê²Œì‹œë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            post_ids, community_key = await self.get_board_posts(page, board_type)
            
            if not post_ids:
                print(f"    âš ï¸ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return
            
            # 2. ê° ê²Œì‹œë¬¼ì—ì„œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            total_files = 0
            for post in post_ids[:3]:  # ìµœëŒ€ 3ê°œ ê²Œì‹œë¬¼
                try:
                    files = await self.download_post_attachments(page, board_type, post['id'], community_key)
                    total_files += files
                    self.results[board_type]['posts'] += 1
                except Exception as e:
                    print(f"    [ERROR] ê²Œì‹œë¬¼ {post['id']} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
            
            self.results[board_type]['files'] = total_files
            print(f"\n  [ì™„ë£Œ] {board_type}: {self.results[board_type]['posts']}ê°œ ê²Œì‹œë¬¼, {total_files}ê°œ íŒŒì¼")
            
        except Exception as e:
            print(f"  [ERROR] {board_type} ê²Œì‹œíŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:100]}")
    
    async def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        await self.initialize()
        
        print("""
        ==============================================================
                    ëª¨ë“  ìë£Œì‹¤ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
        ==============================================================
        
          ëŒ€ìƒ: ì„œì‹, ë²•ë ¹, ì§ˆë¬¸, í†µê³„ ìë£Œì‹¤
          ë°©ì‹: ê° ê²Œì‹œíŒì—ì„œ ìµœëŒ€ 3ê°œ ê²Œì‹œë¬¼ì˜ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        
        ==============================================================
        """)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                slow_mo=500
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                accept_downloads=True
            )
            
            page = await context.new_page()
            
            try:
                # ê° ê²Œì‹œíŒ ì²˜ë¦¬
                for board_type in ['ì„œì‹', 'ë²•ë ¹', 'ì§ˆë¬¸', 'í†µê³„']:
                    await self.scrape_board(page, board_type)
                    await page.wait_for_timeout(2000)  # ê²Œì‹œíŒ ê°„ ëŒ€ê¸°
                
                # ê²°ê³¼ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                result_file = os.path.join(self.download_dir, f"results_{timestamp}.json")
                
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=2)
                
                # ìµœì¢… ê²°ê³¼ ì¶œë ¥
                print(f"""
                ==============================================================
                                    í…ŒìŠ¤íŠ¸ ì™„ë£Œ!
                ==============================================================
                
                ğŸ“Š ì „ì²´ ê²°ê³¼:
                """)
                
                total_posts = 0
                total_files = 0
                
                for board_type, stats in self.results.items():
                    print(f"  {board_type}ìë£Œì‹¤: {stats['posts']}ê°œ ê²Œì‹œë¬¼, {stats['files']}ê°œ íŒŒì¼")
                    total_posts += stats['posts']
                    total_files += stats['files']
                
                print(f"""
                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  ì´ê³„: {total_posts}ê°œ ê²Œì‹œë¬¼, {total_files}ê°œ íŒŒì¼
                
                ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.download_dir}/[ê²Œì‹œíŒ]/post_[ID]/
                ğŸ“„ ê²°ê³¼ íŒŒì¼: {result_file}
                
                ==============================================================
                """)
                
                print("\n[INFO] 10ì´ˆ í›„ ë¸Œë¼ìš°ì €ê°€ ë‹«í™ë‹ˆë‹¤...")
                await page.wait_for_timeout(10000)
                
            except Exception as e:
                print(f"\n[ERROR] {str(e)}")
                import traceback
                traceback.print_exc()
            
            finally:
                await browser.close()
                print("\n[COMPLETE] ì‘ì—… ì™„ë£Œ")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    scraper = AllBoardsScraper()
    await scraper.run()

if __name__ == "__main__":
    print("\nğŸš€ ëª¨ë“  ìë£Œì‹¤ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    asyncio.run(main())